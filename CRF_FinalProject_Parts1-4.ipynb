{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "data_dir = Path(\"data/\")\n",
    "dataset = \"EN\" # EN or ES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1(i): Emission scores\n",
    "Compute $e(x|y) = \\frac{\\text{Count}(y \\rightarrow x)}{\\text{Count}(y)}$ and store it into a dictionary $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_emission_scores(path):\n",
    "    '''\n",
    "    Inputs:\n",
    "        path (Path object or str): path on the local directory to the dataset to load from.\n",
    "    Outputs:\n",
    "        f (dict): Key-value mapping of str(x, y) to log(e(x|y)) values.\n",
    "    '''\n",
    "\n",
    "    count_emission = defaultdict(int) # Stores Count(y -> x), where key is tuple (x, y), and value is Count(y -> x)\n",
    "    count_labels = defaultdict(int) # Stores Count(y), where key is y\n",
    "    \n",
    "    # For computing pairs that are not aligned\n",
    "    unique_x = set()\n",
    "    unique_y = set()\n",
    "    \n",
    "    # Read from dataset path\n",
    "    with open(path) as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "        # Process lines\n",
    "        for line in lines:\n",
    "            # Strip newline\n",
    "            formatted_line = line.strip()\n",
    "            \n",
    "            # Only process lines that are not newlines\n",
    "            if len(formatted_line) > 0:\n",
    "                # Split into (x, y) pair\n",
    "                split_data = formatted_line.split(\" \")\n",
    "                x, y = split_data[0], split_data[1]\n",
    "\n",
    "                count_emission[(x, y)] += 1\n",
    "                count_labels[y] += 1\n",
    "                \n",
    "                unique_x.add(x)\n",
    "                unique_y.add(y)\n",
    "    \n",
    "    # Result dictionary that maps str(x, y) to log(e(x|y)) values\n",
    "    f = {}\n",
    "    \n",
    "    # Estimate e(x|y) based on counts\n",
    "    for x, y in count_emission:\n",
    "        # Create str(x, y)\n",
    "        feature_str = f\"emission:{y}+{x}\"\n",
    "        e = count_emission[(x, y)] / count_labels[y]\n",
    "        \n",
    "        # Store score\n",
    "        f[feature_str] = np.log(e)\n",
    "    \n",
    "    # Set features for emissions that are not aligned\n",
    "    for x in unique_x:\n",
    "        for y in unique_y:\n",
    "            feature_str = f\"emission:{y}+{x}\"\n",
    "            \n",
    "            # Only set to -inf if it doesn't exist as an aligned pair\n",
    "            if feature_str not in f:\n",
    "                f[feature_str] = -10**8\n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1(ii): Transition scores\n",
    "Compute $q(y_i|y_{i-1}) = \\frac{\\text{Count}(y_{i-1}, y_i)}{\\text{Count}(y_{i-1})}$ and store it into a dictionary $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_transition_scores(path):\n",
    "    '''\n",
    "    Inputs:\n",
    "        path (Path object or str): path on the local directory to the dataset to load from.\n",
    "    Outputs:\n",
    "        f (dict): Key-value mapping of str(y_{i-1}, y_i) to log(q(y_i|y_{i-1})) values.\n",
    "    '''\n",
    "    \n",
    "    count_transition = defaultdict(int) # Key is tuple (y_i, y_{i-1}), and value is Count(y_{i-1}, y_i)\n",
    "    count_labels = defaultdict(int) # Stores Count(y_i), where key is y_i\n",
    "    \n",
    "    # For computing pairs that are not aligned\n",
    "    unique_y = set([\"START\", \"STOP\"])\n",
    "\n",
    "    # Read from dataset path\n",
    "    with open(path) as f:\n",
    "        lines = f.readlines()\n",
    "        # Initialize prev_y as START\n",
    "        prev_y = \"START\"\n",
    "        \n",
    "        # Process lines\n",
    "        for line in lines:\n",
    "            # Strip newline\n",
    "            formatted_line = line.strip()\n",
    "            \n",
    "            # Only process lines that are not newlines\n",
    "            if len(formatted_line) > 0:\n",
    "                # Split into (x, y) pair\n",
    "                split_data = formatted_line.split(\" \")\n",
    "                x, y = split_data[0], split_data[1]\n",
    "                \n",
    "                transition_key = (prev_y, y)\n",
    "                count_transition[transition_key] += 1\n",
    "                count_labels[y] += 1\n",
    "                \n",
    "                # Update for next word\n",
    "                prev_y = y\n",
    "                \n",
    "                unique_y.add(y)\n",
    "            else:\n",
    "                # End of sentence\n",
    "                # Store Count(STOP|y_n)\n",
    "                transition_key = (prev_y, \"STOP\")\n",
    "                count_transition[transition_key] += 1\n",
    "                \n",
    "                # Start of next sentence: initialize prev_y to \"START\" and store Count(START)\n",
    "                prev_y = \"START\"\n",
    "                count_labels[prev_y] += 1\n",
    "    \n",
    "    # Result dictionary that maps str(x, y) to log(e(x|y)) values\n",
    "    f = {}\n",
    "    \n",
    "    # Estimate e(x|y) based on counts\n",
    "    for prev_y, y in count_transition:\n",
    "        # Create str(y_{i-1}, y_i)\n",
    "        feature_str = f\"transition:{prev_y}+{y}\"\n",
    "        q = count_transition[(prev_y, y)] / count_labels[prev_y]\n",
    "        \n",
    "        # Store score\n",
    "        f[feature_str] = np.log(q)\n",
    "    \n",
    "    # Set features for transitions that are not aligned\n",
    "    for prev_y in unique_y:\n",
    "        for y in unique_y:\n",
    "            feature_str = f\"transition:{prev_y}+{y}\"\n",
    "            \n",
    "            # Only set to -inf if it doesn't exist as an aligned pair\n",
    "            if feature_str not in f:\n",
    "                f[feature_str] = -10**8\n",
    "\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute emission and transition parameters\n",
    "f_emission_train = calculate_emission_scores(data_dir / dataset / \"train\")\n",
    "f_transition_train = calculate_transition_scores(data_dir / dataset / \"train\")\n",
    "\n",
    "# Combine the transition and emission dictionaries together\n",
    "f = {**f_emission_train, **f_transition_train}\n",
    "# Ensure the number of elements is correct\n",
    "assert(len(f) == len(f_emission_train) + len(f_transition_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2(i)\n",
    "Compute CRF scores for a given input and output sequence pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_crf_score(x, y, feature_dict):\n",
    "    ''' \n",
    "    Inputs:\n",
    "        x (list[str]): Complete input word sentence (without START or STOP tags)\n",
    "        y (list[str]): Complete output label sequence\n",
    "        feature_dict (dict[str] -> float): Dictionary that maps a given feature to its score.\n",
    "    Outputs:\n",
    "        p (float): Score given by p(y | x)\n",
    "    '''\n",
    "    \n",
    "    # Input and output sequences must be of the same length\n",
    "    assert(len(x) == len(y))\n",
    "    n = len(x) # Sequence length\n",
    "    \n",
    "    # Stores the number of times each feature appears in (x, y)\n",
    "    feature_count = defaultdict(int)\n",
    "    \n",
    "    # Compute emission features\n",
    "    for i in range(n):\n",
    "        formatted_word = x[i]\n",
    "        emission_key = f\"emission:{y[i]}+{formatted_word}\"\n",
    "        feature_count[emission_key] += 1\n",
    "    \n",
    "    # Compute transition features\n",
    "    # Add START and STOP tags to y\n",
    "    updated_y = [\"START\"] + y + [\"STOP\"]\n",
    "    for i in range(1, n+2):\n",
    "        prev_y = updated_y[i-1]\n",
    "        y_i = updated_y[i]\n",
    "        transition_key = f\"transition:{prev_y}+{y_i}\"\n",
    "        feature_count[transition_key] += 1\n",
    "    \n",
    "    # Compute score\n",
    "    score = 0\n",
    "    for feature_key, count in feature_count.items():\n",
    "        weight = feature_dict[feature_key]\n",
    "        score += weight * count\n",
    "    \n",
    "    return score\n",
    "\n",
    "# # Test function\n",
    "# x = \"Great food with an awesome atmosphere !\".split()\n",
    "# y = \"O B-positive O O O B-positive O\".split()\n",
    "# compute_crf_score(x, y, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2(ii)\n",
    "Viterbi algorithm for decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_states(path):\n",
    "    '''\n",
    "    Inputs:\n",
    "        path (Path object or str): path on the local directory to the dataset to load from.\n",
    "    Outputs:\n",
    "        states (list[str]): Unique states in the dataset.\n",
    "    '''\n",
    "    states = set()\n",
    "    \n",
    "    # Read from dataset path\n",
    "    with open(path) as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "        # Process lines\n",
    "        for line in lines:\n",
    "            # Strip newline\n",
    "            formatted_line = line.strip()\n",
    "            \n",
    "            # Only process lines that are not newlines\n",
    "            if len(formatted_line) > 0:\n",
    "                # Split into (x, y) pair\n",
    "                split_data = formatted_line.split(\" \")\n",
    "                x, y = split_data[0], split_data[1]\n",
    "                states.add(y)\n",
    "    \n",
    "    return list(states)\n",
    "\n",
    "def viterbi_decode(x, states, feature_dict, default_index=0):\n",
    "    '''\n",
    "    Inputs:\n",
    "        x (list[str]): Input sequence.\n",
    "        states (list[str]): Possible output states.\n",
    "        feature_dict (dict[str] -> float): Dictionary that maps a given feature to its score.\n",
    "        default_index (int, optional): Index to default for backpointer if scores are nan.\n",
    "    Outputs:\n",
    "        y (list[str]): Most probable output sequence.\n",
    "    '''\n",
    "    \n",
    "    n = len(x) # Number of words\n",
    "    d = len(states) # Number of states\n",
    "    scores = np.full((n, d), -np.inf) # Default state is -inf for missing values\n",
    "    bp = np.full((n, d), default_index, dtype=np.int) # Set default backpointer to the default_index state\n",
    "\n",
    "    # Convert to lowercase\n",
    "    x = [x[i] for i in range(n)]\n",
    "    \n",
    "    # Compute START transition scores\n",
    "    for i, current_y in enumerate(states):\n",
    "        transition_key = f\"transition:START+{current_y}\"\n",
    "        emission_key = f\"emission:{current_y}+{x[0]}\"\n",
    "        transmission_score = feature_dict.get(transition_key, -10**8)\n",
    "        emission_score = feature_dict.get(emission_key, -10**8)\n",
    "        scores[0, i] = transmission_score + emission_score\n",
    "    \n",
    "    # Recursively compute best scores based on transmission and emission scores at each node\n",
    "    for i in range(1, n):\n",
    "        for k, prev_y in enumerate(states):\n",
    "            for j, current_y in enumerate(states):\n",
    "                transition_key = f\"transition:{prev_y}+{current_y}\"\n",
    "                emission_key = f\"emission:{current_y}+{x[i]}\"\n",
    "                \n",
    "                transition_score = feature_dict.get(transition_key, -10**8)\n",
    "                emission_score = feature_dict.get(emission_key, -10**8)\n",
    "                overall_score = emission_score + transition_score + scores[i-1, k]\n",
    "\n",
    "                # Better score is found: Update backpointer and score arrays\n",
    "                if overall_score > scores[i, j]:\n",
    "                    scores[i, j] = overall_score\n",
    "                    bp[i,j] = k\n",
    "    \n",
    "    # Compute for STOP\n",
    "    highest_score = None\n",
    "    highest_bp = default_index\n",
    "    \n",
    "    for j, prev_y in enumerate(states):\n",
    "        transition_key = f\"transition:{prev_y}+STOP\"\n",
    "        transition_score = feature_dict.get(transition_key, -10**8)\n",
    "        overall_score = transition_score + scores[n-1, j]\n",
    "        \n",
    "        if highest_score == None or overall_score > highest_score:\n",
    "            highest_score = overall_score\n",
    "            highest_bp = j\n",
    "    \n",
    "    # Follow backpointers to get output sequence\n",
    "    result = [states[highest_bp]]\n",
    "    prev_bp = highest_bp\n",
    "    for i in range(n-1, 0, -1):\n",
    "        prev_bp = bp[i, prev_bp]\n",
    "        output = states[prev_bp]\n",
    "        # Prepend result to output list\n",
    "        result = [output] + result\n",
    "    \n",
    "    return result\n",
    "\n",
    "states = get_states(data_dir / dataset / \"train\")\n",
    "# viterbi_decode(\"Great food with an awesome atmosphere !\".split(), states, f) # Should be similar or equal to y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform decoding on dev sets\n",
    "def inference(path, states, feature_dict,output_type):\n",
    "    '''\n",
    "    Given a path, perform inference on sentences in the dataset and writes it to disk.\n",
    "    \n",
    "    Inputs:\n",
    "        path (Path object or str): path on the local directory to the dataset to perform inference on.\n",
    "        states (list[str]): Unique states that can be predicted.\n",
    "        feature_dict (dict[str] -> float): Dictionary that maps a given feature to its score.\n",
    "    Outputs:\n",
    "        None\n",
    "    '''\n",
    "    \n",
    "    default_index = states.index('O')\n",
    "    sentences = []\n",
    "\n",
    "    # Write predictions to file\n",
    "    output_filename = str(path).replace(\".in\", \".{}.out\".format(output_type))\n",
    "\n",
    "    # Read from dataset path\n",
    "    with open(path) as f:\n",
    "        lines = f.readlines()\n",
    "        sentence = []\n",
    "        \n",
    "        for line in lines:\n",
    "            formatted_line = line.strip()\n",
    "            \n",
    "            # Not the end of sentence, add it to the list\n",
    "            if len(formatted_line) > 0:\n",
    "                sentence.append(formatted_line)\n",
    "            else:\n",
    "                # End of sentence\n",
    "                sentences.append(sentence)\n",
    "                sentence = []\n",
    "    \n",
    "    # Write output file\n",
    "    with open(output_filename, \"w\") as wf:\n",
    "        for sentence in sentences:\n",
    "            # Run predictions\n",
    "            pred_sentence = viterbi_decode(sentence, states, feature_dict, default_index)\n",
    "            \n",
    "            # Write original word and predicted tags\n",
    "            for i in range(len(sentence)):\n",
    "                wf.write(sentence[i] + \" \" + pred_sentence[i] + \"\\n\")\n",
    "            \n",
    "            # End of sentence, write newline\n",
    "            wf.write(\"\\n\")\n",
    "\n",
    "inference(data_dir / dataset / \"dev.in\", states, f, output_type='p2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3(i)\n",
    "Loss calculation using forward algorithm. We first define the score calculation functions for a single sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-45.37727632870497"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_forward_score(x, feature_dict, states):\n",
    "    '''\n",
    "    Uses the forward algorithm to compute the score for a given sequence.\n",
    "    \n",
    "    Inputs:\n",
    "        x (list[str]): Input sequence.\n",
    "        feature_dict (dict[str] -> float): Dictionary that maps a given feature to its score.\n",
    "        states (list[str]): List of unique states.\n",
    "    Outputs:\n",
    "        forward_score (float): Forward score for this sequence.\n",
    "    '''\n",
    "    \n",
    "    n = len(x) # Number of words\n",
    "    d = len(states) # Number of states\n",
    "    forward_scores = np.zeros((n, d))\n",
    "    \n",
    "    # Start forward pass\n",
    "    # Compute START transition scores\n",
    "    for i, current_y in enumerate(states):\n",
    "        transition_key = f\"transition:START+{current_y}\"\n",
    "        emission_key = f\"emission:{current_y}+{x[0]}\"\n",
    "        transition_score = feature_dict.get(transition_key, -10**8)\n",
    "        emission_score = feature_dict.get(emission_key, -10**8)\n",
    "        # Sum exponentials\n",
    "        forward_scores[0, i] = transition_score + emission_score\n",
    "    \n",
    "    # Recursively compute best scores based on transmission and emission scores at each node\n",
    "    for i in range(1, n):\n",
    "        for k, current_y in enumerate(states):\n",
    "            temp_score = 0\n",
    "            for j, prev_y in enumerate(states):\n",
    "                transition_key = f\"transition:{prev_y}+{current_y}\"\n",
    "                emission_key = f\"emission:{current_y}+{x[i]}\"\n",
    "                \n",
    "                transition_score = feature_dict.get(transition_key, -10**8)\n",
    "                emission_score = feature_dict.get(emission_key, -10**8)\n",
    "                \n",
    "                # Sum exponentials\n",
    "                temp_score += np.exp(min(emission_score + transition_score + forward_scores[i-1, j], 700))\n",
    "\n",
    "            # Add to forward scores array\n",
    "            forward_scores[i, k] = np.log(temp_score) if temp_score else -10**8\n",
    "\n",
    "    # Compute for STOP\n",
    "    forward_prob = 0\n",
    "    for j, prev_y in enumerate(states):\n",
    "        transition_key = f\"transition:{prev_y}+STOP\"\n",
    "        transition_score = feature_dict.get(transition_key, -10**8)\n",
    "        # Sum exponentials\n",
    "        overall_score = np.exp(min(transition_score + forward_scores[n-1, j], 700))\n",
    "        forward_prob += overall_score\n",
    "    # End forward pass\n",
    "    \n",
    "    alpha = np.log(forward_prob) if forward_prob else -700\n",
    "    return forward_scores, alpha\n",
    "\n",
    "\n",
    "def compute_crf_loss_sample(x, y, feature_dict, states):\n",
    "    '''\n",
    "    Inputs:\n",
    "        x (list[str]): Input sequence.\n",
    "        y (list[str]): Groundtruth output sequence.\n",
    "        feature_dict (dict[str] -> float): Dictionary that maps a given feature to its score.\n",
    "        states (list[str]): List of unique states.\n",
    "    Outputs:\n",
    "        loss (float): Loss value for a single sample.\n",
    "    '''\n",
    "    first_term = compute_crf_score(x, y, feature_dict)\n",
    "    _, forward_score = compute_forward_score(x, feature_dict, states)\n",
    "    \n",
    "    loss = -(first_term - forward_score)\n",
    "    return loss\n",
    "\n",
    "# -45.37727\n",
    "_, a = compute_forward_score(\"Great food with an awesome atmosphere !\".split(), f, states)\n",
    "a\n",
    "# compute_forward_score(['hola', 'buenas', 'agradezco', 'a', 'todas', 'las', 'personas', 'que', 'hayan', 'puesto', 'estos', 'comentarios', 'porque', 'esto', 'la', 'verdad', 'te', 'deja', 'saber', 'lo', 'bueno', 'y', 'lo', 'malo', 'que', 'tiene', 'el', 'restaurante', 'pero', 'yo', 'recomiendo', 'a', 'todas', 'las', 'personas', 'este', 'restaurante', 'porque', 'muy', 'pocos', 'restaurantes', 'en', 'zaragoza', 'tienen', 'pruductos', 'que', 'tiene', 'el', 'puerto', 'como', 'por', 'ejemplo', 'el', 'pescado', 'es', 'de', 'muy', 'alta', 'calidad', 'y', 'la', 'carne', 'es', 'muy', 'buena', 'que', 'es', 'de', 'denominacion', 'de', 'origen', 'y', 'hay', 'platos', 'muy', 'sabrosos', 'y', 'lo', 'bueno', 'que', 'tiene', 'el', 'restaurante', 'es', 'que', 'prepara', 'sus', 'deleciosos', 'platos', 'al', 'momento', 'nunca', 'se', 'prepara', 'la', 'comida', 'y', 'luego', 'se', 'calienta', 'y', 'con', 'un', 'tiempo', 'justo', 'para', 'el', 'cliente', 'y', 'con', 'mucha', 'limpieza', 'en', 'cocina', 'y', 'con', 'mucho', 'cuidado', 'con', 'el', 'pescado', 'y', 'demas', 'pruductos', 'y', 'con', 'excelentes', 'postres', 'caseros', 'como', 'la', 'quesa', 'pasiega', ',', 'pudin', 'de', 'frutas', ',', 'bolas', 'de', 'melon', 'maceradas', 'en', 'vino', 'de', 'pedro', 'xmenez', ',', 'tarta', 'imperial', 'de', 'almendras', 'y', 'chocolate', ',', 'etc', '.'], f, states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the methods defined early to compute the loss across the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(path):\n",
    "    '''\n",
    "    Given a path, load the data from file.\n",
    "    \n",
    "    Inputs:\n",
    "        path (Path object or str): path on the local directory to the dataset to perform inference on.\n",
    "    Outputs:\n",
    "        input_sequences (list[list[str]]): List of input sequences\n",
    "        input_labels (list[list[str]]): List of input labels\n",
    "    '''\n",
    "    input_sequences = []\n",
    "    input_labels = []\n",
    "    \n",
    "    # Read from dataset path\n",
    "    with open(path) as f:\n",
    "        lines = f.readlines()\n",
    "        sentence = []\n",
    "        labels = []\n",
    "        \n",
    "        for line in lines:\n",
    "            formatted_line = line.strip()\n",
    "            \n",
    "            # Not the end of sentence, add it to the list\n",
    "            if len(formatted_line) > 0:\n",
    "                split_data = formatted_line.split(\" \")\n",
    "                x, y = split_data[0], split_data[1]\n",
    "\n",
    "                sentence.append(x)\n",
    "                labels.append(y)\n",
    "            else:\n",
    "                # End of sentence\n",
    "                input_sequences.append(sentence)\n",
    "                input_labels.append(labels)\n",
    "                sentence = []\n",
    "                labels = []\n",
    "    \n",
    "    return input_sequences, input_labels\n",
    "\n",
    "def compute_crf_loss(input_sequences, input_labels, feature_dict, states, nabla=0, regularization= False):\n",
    "    '''\n",
    "    Given a path, perform inference on sentences in the dataset and writes it to disk.\n",
    "    \n",
    "    Inputs:\n",
    "        input_sequences (list[list[str]]): List of input sequences\n",
    "        input_labels (list[list[str]]): List of input labels\n",
    "        feature_dict (dict[str] -> float): Dictionary that maps a given feature to its score.\n",
    "        states (list[str]): List of unique states.\n",
    "    Outputs:\n",
    "        loss (float): Loss value for the dataset.\n",
    "    '''\n",
    "    loss = 0\n",
    "    for i in range(len(input_sequences)):\n",
    "        sample_loss = compute_crf_loss_sample(input_sequences[i], input_labels[i], feature_dict, states)\n",
    "        loss += sample_loss\n",
    "    if regularization:\n",
    "        reg_loss = 0\n",
    "        for feature_key in feature_dict:\n",
    "            reg_loss += feature_dict[feature_key]**2\n",
    "        reg_loss = nabla*reg_loss\n",
    "        loss += reg_loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3(ii)\n",
    "Forward backward algorithm for computing gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7210505872358333"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_backward_score(x, feature_dict, states):\n",
    "    '''\n",
    "    Uses the backward algorithm to compute the score for a given sequence.\n",
    "    \n",
    "    Inputs:\n",
    "        x (list[str]): Input sequence.\n",
    "        feature_dict (dict[str] -> float): Dictionary that maps a given feature to its score.\n",
    "        states (list[str]): List of unique states.\n",
    "    Outputs:\n",
    "        backward_score (float): Forward score for this sequence.\n",
    "    '''\n",
    "    n = len(x) # Number of words\n",
    "    d = len(states) # Number of states\n",
    "    backward_scores = np.zeros((n, d))\n",
    "    \n",
    "    # Start backward pass\n",
    "    # Compute STOP transition scores\n",
    "    for j, current_y in enumerate(states):\n",
    "        transition_key = f\"transition:{current_y}+STOP\"\n",
    "        transition_score = feature_dict.get(transition_key, -10**8)\n",
    "        # Sum exponentials\n",
    "        backward_scores[n-1, j] = transition_score\n",
    "\n",
    "    # Recursively compute best scores based on transmission and emission scores at each node\n",
    "    for i in range(n-1, 0, -1):\n",
    "        for k, current_y in enumerate(states):\n",
    "            temp_score = 0\n",
    "            for j, next_y in enumerate(states):\n",
    "                transition_key = f\"transition:{current_y}+{next_y}\"\n",
    "                emission_key = f\"emission:{next_y}+{x[i]}\"\n",
    "                \n",
    "                transition_score = feature_dict.get(transition_key, -10**8)\n",
    "                emission_score = feature_dict.get(emission_key, -10**8)\n",
    "\n",
    "                # Sum exponentials\n",
    "                temp_score += np.exp(min(emission_score + transition_score + backward_scores[i, j], 700))\n",
    "\n",
    "            # Add to backward scores array\n",
    "            backward_scores[i-1, k] = np.log(temp_score) if temp_score else -10**8\n",
    "    \n",
    "    # Compute for START\n",
    "    backward_prob = 0\n",
    "    for j, next_y in enumerate(states):\n",
    "        transition_key = f\"transition:START+{next_y}\"\n",
    "        emission_key = f\"emission:{next_y}+{x[0]}\" # Emission of last word\n",
    "\n",
    "        transition_score = feature_dict.get(transition_key, -10**8)\n",
    "        emission_score = feature_dict.get(emission_key, -10**8)\n",
    "        # Sum exponentials\n",
    "        overall_score = np.exp(min(emission_score + transition_score + backward_scores[0, j], 700))\n",
    "        backward_prob += overall_score\n",
    "    # End backward pass\n",
    "    \n",
    "    beta = np.log(backward_prob) if backward_prob else -700\n",
    "    return backward_scores, beta\n",
    "\n",
    "def forward_backward(x, feature_dict, states):\n",
    "    '''\n",
    "    Uses the forward-backward algorithm to compute the expected counts for a given sequence.\n",
    "    \n",
    "    Inputs:\n",
    "        x (list[str]): Input sequence.\n",
    "        feature_dict (dict[str] -> float): Dictionary that maps a given feature to its score.\n",
    "        states (list[str]): List of unique states.\n",
    "    Outputs:\n",
    "        expected_counts (dict): Expected counts for each feature.\n",
    "    '''\n",
    "    \n",
    "    n = len(x) # Number of words\n",
    "    d = len(states) # Number of states\n",
    "    \n",
    "    forward_scores, alpha = compute_forward_score(x, feature_dict, states)\n",
    "    forward_prob = np.exp(min(alpha, 700))\n",
    "    backward_scores, beta = compute_backward_score(x, feature_dict, states)\n",
    "    backward_prob = np.exp(min(beta, 700))\n",
    "    \n",
    "    # Computed expected counts for all features\n",
    "    feature_expected_counts = defaultdict(float)\n",
    "    \n",
    "    # Compute expected emission counts\n",
    "    for i in range(n):\n",
    "        for j, current_y in enumerate(states):\n",
    "            emission_key = f\"emission:{current_y}+{x[i]}\"\n",
    "            feature_expected_counts[emission_key] += np.exp(min(forward_scores[i, j] + backward_scores[i, j] - alpha, 700))\n",
    "    \n",
    "    # Compute expected START / STOP transition counts\n",
    "    for j, next_y in enumerate(states):\n",
    "        start_transition_key = f\"transition:START+{next_y}\"\n",
    "        feature_expected_counts[start_transition_key] += np.exp(min(forward_scores[0, j] + backward_scores[0, j] - alpha, 700))\n",
    "        \n",
    "        stop_transition_key = f\"transition:{next_y}+STOP\"\n",
    "        feature_expected_counts[stop_transition_key] += np.exp(min(forward_scores[n-1, j] + backward_scores[n-1, j] - alpha, 700))\n",
    "    \n",
    "    # Compute expected transition probabilities for everything else\n",
    "    for j, current_y in enumerate(states):\n",
    "        for k, next_y in enumerate(states):\n",
    "            transition_key = f\"transition:{current_y}+{next_y}\"\n",
    "            transition_score = feature_dict.get(transition_key, -10**8)\n",
    "            \n",
    "            total = 0\n",
    "            for i in range(n-1):\n",
    "                emission_key = f\"emission:{next_y}+{x[i+1]}\"\n",
    "                emission_score = feature_dict.get(emission_key, -10**8)\n",
    "\n",
    "                total += np.exp(min(forward_scores[i, j] + backward_scores[i+1, k] + transition_score + emission_score - alpha, 700))\n",
    "\n",
    "            feature_expected_counts[transition_key] = total\n",
    "    \n",
    "    return feature_expected_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_count(x, y, feature_dict):\n",
    "    ''' \n",
    "    Inputs:\n",
    "        x (list[str]): Complete input word sentence (without START or STOP tags)\n",
    "        y (list[str]): Complete output label sequence\n",
    "        feature_dict (dict[str] -> float): Dictionary that maps a given feature to its score.\n",
    "    Outputs:\n",
    "        p (float): Score given by p(y | x)\n",
    "    '''\n",
    "    \n",
    "    # Input and output sequences must be of the same length\n",
    "    assert(len(x) == len(y))\n",
    "    n = len(x) # Sequence length\n",
    "    \n",
    "    # Stores the number of times each feature appears in (x, y)\n",
    "    feature_count = defaultdict(int)\n",
    "    \n",
    "    # Compute emission features\n",
    "    for i in range(n):\n",
    "        formatted_word = x[i]\n",
    "        emission_key = f\"emission:{y[i]}+{formatted_word}\"\n",
    "        feature_count[emission_key] += 1\n",
    "    \n",
    "    # Compute transition features\n",
    "    # Add START and STOP tags to y\n",
    "    updated_y = [\"START\"] + y + [\"STOP\"]\n",
    "    for i in range(1, n+2):\n",
    "        prev_y = updated_y[i-1]\n",
    "        y_i = updated_y[i]\n",
    "        transition_key = f\"transition:{prev_y}+{y_i}\"\n",
    "        feature_count[transition_key] += 1\n",
    "    \n",
    "    return feature_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wish to compute the gradient:\n",
    "$\\frac{\\delta L(w)}{\\delta \\lambda_k} = \\sum_i E_{p(y|x_i)} [ f_k (x_i, y) ] - \\sum_i f_k (x_i, y_i) $\n",
    "\n",
    "which can be computed by subtracting the actual counts of the features from the expected counts of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_gradients(train_inputs, train_labels, f, states, nabla = 0, regularization = False):\n",
    "    '''\n",
    "    Uses the forward-backward algorithm to compute gradients analytically.\n",
    "    \n",
    "    Inputs:\n",
    "        train_inputs (list[str]): Input sequence.\n",
    "        train_labels (list[str]): Input labels.\n",
    "        f (dict[str] -> float): Dictionary that maps a given feature to its score.\n",
    "        states (list[str]): List of unique states.\n",
    "    Outputs:\n",
    "        backward_score (float): Backward score for this sequence.\n",
    "        feature_gradients (dict[str] -> float): Dictionary that maps a given feature to its analytical gradient.\n",
    "    '''\n",
    "    \n",
    "    feature_gradients = defaultdict(float)\n",
    "\n",
    "    for i in range(len(train_labels)):\n",
    "        x = train_inputs[i]\n",
    "        y = train_labels[i]\n",
    "\n",
    "        feature_expected_counts = forward_backward(x, f, states)\n",
    "        actual_counts = get_feature_count(x, y, f)\n",
    "\n",
    "        for k, v in feature_expected_counts.items():\n",
    "            feature_gradients[k] += v\n",
    "\n",
    "        for k, v in actual_counts.items():\n",
    "            feature_gradients[k] -= v\n",
    "        \n",
    "    if regularization:\n",
    "        for k, v in f.items():\n",
    "            feature_gradients[k] += 2*nabla*f[k]\n",
    "\n",
    "    return feature_gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the results numerically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking emission:O+the\n",
      "Checking transition:START+O\n",
      "Checking transition:O+O\n",
      "Checking transition:I-negative+I-positive\n"
     ]
    }
   ],
   "source": [
    "states = get_states(data_dir / dataset / \"train\")\n",
    "train_inputs, train_labels = get_dataset(data_dir / dataset / \"train\")\n",
    "\n",
    "# Comment the below section out for Spanish, as O+the doesn't exist!\n",
    "# Check against numerical gradient for several values\n",
    "feature_key_checks = ['emission:O+the', 'transition:START+O', 'transition:O+O', 'transition:I-negative+I-positive']\n",
    "feature_gradients = compute_gradients(train_inputs, train_labels, f, states)\n",
    "loss1 = compute_crf_loss(train_inputs, train_labels, f, states)\n",
    "delta = 1e-6\n",
    "\n",
    "for feature_key in feature_key_checks:\n",
    "    print(\"Checking\", feature_key)\n",
    "    new_f = f.copy()\n",
    "    new_f[feature_key] += delta\n",
    "\n",
    "    loss2 = compute_crf_loss(train_inputs, train_labels, new_f, states)\n",
    "\n",
    "    numerical_gradient = (loss2 - loss1) / delta\n",
    "    analytical_gradient = feature_gradients[feature_key]\n",
    "    \n",
    "    # Ensure numerical and analytical gradient are close\n",
    "    assert(abs(numerical_gradient - analytical_gradient) / max(abs(numerical_gradient), 1e-8) <= 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are helper functions for dictionary to array conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_to_dict(w, f, reverse = False):\n",
    "    '''\n",
    "    Converts a numpy array w to a dictionary with keys from f.\n",
    "    '''\n",
    "    for i,k in enumerate(f.keys()):\n",
    "        f[k] = w[i]\n",
    "    return f\n",
    "def prepare_grad_for_bfgs(grads,f):\n",
    "    '''\n",
    "    Converts a dictionary to a numpy array.\n",
    "    '''\n",
    "    np_grads = np.zeros(len(f))\n",
    "    for i,k in enumerate(f.keys()):\n",
    "        np_grads[i] = grads[k]\n",
    "    return np_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running L-BFGS on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:19007.6578\n",
      "Loss:12068.6971\n",
      "Loss:11393.8658\n",
      "Loss:10788.1557\n",
      "Loss:10197.2511\n",
      "Loss:9635.8611\n",
      "Loss:8861.8972\n",
      "Loss:8284.0767\n",
      "Loss:8078.5501\n",
      "Loss:7887.9522\n",
      "Loss:7669.0499\n",
      "Loss:7304.0618\n",
      "Loss:6878.9758\n",
      "Loss:6513.3204\n",
      "Loss:6348.9892\n",
      "Loss:6077.3882\n",
      "Loss:5959.0012\n",
      "Loss:5657.8769\n",
      "Loss:5434.7521\n",
      "Loss:5381.2663\n",
      "Loss:5333.5624\n",
      "Loss:5302.8242\n",
      "Loss:5214.8213\n",
      "Loss:5109.0913\n",
      "Loss:4954.7691\n",
      "Loss:4904.4185\n",
      "Loss:4758.8482\n",
      "Loss:4651.4197\n",
      "Loss:4557.8960\n",
      "Loss:4489.4024\n",
      "Loss:4391.6532\n",
      "Loss:4344.1131\n",
      "Loss:4210.4801\n",
      "Loss:4105.6601\n",
      "Loss:4075.4750\n",
      "Loss:4021.3417\n",
      "Loss:3972.4631\n",
      "Loss:3928.7447\n",
      "Loss:3883.9005\n",
      "Loss:3868.3786\n",
      "Loss:3817.6166\n",
      "Loss:3794.7724\n",
      "Loss:3737.7940\n",
      "Loss:3677.2720\n",
      "Loss:3645.2902\n",
      "Loss:3629.3420\n",
      "Loss:3590.4545\n",
      "Loss:3524.2284\n",
      "Loss:3504.9559\n",
      "Loss:3481.1172\n",
      "Loss:3456.0795\n",
      "Loss:3430.1995\n",
      "Loss:3392.3634\n",
      "Loss:3380.2429\n",
      "Loss:3359.1886\n",
      "Loss:3351.9971\n",
      "Loss:3335.6018\n",
      "Loss:3315.2665\n",
      "Loss:3291.1134\n",
      "Loss:3272.1581\n",
      "Loss:3257.8409\n",
      "Loss:3233.6885\n",
      "Loss:3221.0961\n",
      "Loss:3208.6069\n",
      "Loss:3191.0252\n",
      "Loss:3181.0028\n",
      "Loss:3169.0722\n",
      "Loss:3160.6006\n",
      "Loss:3154.9179\n",
      "Loss:3149.8268\n",
      "Loss:3140.7799\n",
      "Loss:3135.4668\n",
      "Loss:3127.1011\n",
      "Loss:3120.1909\n",
      "Loss:3112.6164\n",
      "Loss:3107.6346\n",
      "Loss:3096.4820\n",
      "Loss:3091.5554\n",
      "Loss:3086.2611\n",
      "Loss:3083.6892\n",
      "Loss:3079.9721\n",
      "Loss:3076.7777\n",
      "Loss:3074.5204\n",
      "Loss:3067.1904\n",
      "Loss:3061.0245\n",
      "Loss:3057.2642\n",
      "Loss:3054.5564\n",
      "Loss:3049.8131\n",
      "Loss:3047.3481\n",
      "Loss:3045.1105\n",
      "Loss:3043.7519\n",
      "Loss:3042.8289\n",
      "Loss:3041.8156\n",
      "Loss:3041.2108\n",
      "Loss:3040.1734\n",
      "Loss:3037.4382\n",
      "Loss:3036.5460\n",
      "Loss:3035.5842\n",
      "Loss:3034.0950\n",
      "Loss:3032.1838\n",
      "Loss:3030.7703\n",
      "Loss:3029.7850\n",
      "Loss:3029.3183\n",
      "Loss:3027.9895\n",
      "Loss:3026.8130\n",
      "Loss:3026.0401\n",
      "Loss:3025.3183\n",
      "Loss:3024.5718\n",
      "Loss:3024.4724\n",
      "Loss:3023.9109\n",
      "Loss:3023.8248\n",
      "Loss:3023.3837\n",
      "Loss:3022.7564\n",
      "Loss:3022.2668\n",
      "Loss:3021.7766\n",
      "Loss:3021.5711\n",
      "Loss:3021.4454\n",
      "Loss:3021.2709\n",
      "Loss:3020.8119\n",
      "Loss:3020.4929\n",
      "Loss:3020.3034\n",
      "Loss:3020.1394\n",
      "Loss:3019.9640\n",
      "Loss:3019.7188\n",
      "Loss:3019.4749\n",
      "Loss:3019.3040\n",
      "Loss:3019.1834\n",
      "Loss:3019.0529\n",
      "Loss:3018.9404\n",
      "Loss:3018.8622\n",
      "Loss:3018.5998\n",
      "Loss:3018.4824\n",
      "Loss:3018.3316\n",
      "Loss:3018.1612\n",
      "Loss:3017.9972\n",
      "Loss:3017.7906\n",
      "Loss:3017.6870\n",
      "Loss:3017.5863\n",
      "Loss:3017.5281\n",
      "Loss:3017.4814\n",
      "Loss:3017.4182\n",
      "Loss:3017.3102\n",
      "Loss:3017.2023\n",
      "Loss:3017.1234\n",
      "Loss:3017.0234\n",
      "Loss:3016.9810\n",
      "Loss:3016.8631\n",
      "Loss:3016.7800\n",
      "Loss:3016.6898\n",
      "Loss:3016.6205\n",
      "Loss:3016.5737\n",
      "Loss:3016.5408\n",
      "Loss:3016.5151\n",
      "Loss:3016.4038\n",
      "Loss:3016.3278\n",
      "Loss:3016.2776\n",
      "Loss:3016.2507\n",
      "Loss:3016.2128\n",
      "Loss:3016.1983\n",
      "Loss:3016.1756\n",
      "Loss:3016.1411\n",
      "Loss:3016.0799\n",
      "Loss:3016.0570\n",
      "Loss:3016.0252\n",
      "Loss:3016.0047\n",
      "Loss:3015.9905\n",
      "Loss:3015.9556\n",
      "Loss:3015.9377\n",
      "Loss:3015.9229\n",
      "Loss:3015.9138\n",
      "Loss:3015.9027\n",
      "Loss:3015.8904\n",
      "Loss:3015.8673\n",
      "Loss:3015.8562\n",
      "Loss:3015.8483\n",
      "Loss:3015.8382\n",
      "Loss:3015.8234\n",
      "Loss:3015.8113\n",
      "Loss:3015.8037\n",
      "Loss:3015.7898\n",
      "Loss:3015.7834\n",
      "Loss:3015.7756\n",
      "Loss:3015.7659\n",
      "Loss:3015.7596\n",
      "Loss:3015.7515\n",
      "Loss:3015.7459\n",
      "Loss:3015.7378\n",
      "Loss:3015.7328\n",
      "Loss:3015.7290\n",
      "Loss:3015.7217\n",
      "Loss:3015.7161\n",
      "Loss:3015.7084\n",
      "Loss:3015.7002\n",
      "Loss:3015.6924\n",
      "Loss:3015.6857\n",
      "Loss:3015.6787\n",
      "Loss:3015.6709\n",
      "Loss:3015.6661\n",
      "Loss:3015.6626\n",
      "Loss:3015.6597\n",
      "Loss:3015.6565\n",
      "Loss:3015.6518\n",
      "Loss:3015.6487\n",
      "Loss:3015.6437\n",
      "Loss:3015.6412\n",
      "Loss:3015.6397\n",
      "Loss:3015.6375\n",
      "Loss:3015.6359\n",
      "Loss:3015.6320\n",
      "Loss:3015.6283\n",
      "Loss:3015.6241\n",
      "Loss:3015.6192\n",
      "Loss:3015.6162\n",
      "Loss:3015.6137\n",
      "Loss:3015.6121\n",
      "Loss:3015.6112\n",
      "Loss:3015.6100\n",
      "Loss:3015.6088\n",
      "Loss:3015.6080\n",
      "Loss:3015.6070\n",
      "Loss:3015.6050\n",
      "Loss:3015.6024\n",
      "Loss:3015.6019\n",
      "Loss:3015.5991\n",
      "Loss:3015.5985\n",
      "Loss:3015.5979\n",
      "Loss:3015.5973\n",
      "Loss:3015.5961\n",
      "Loss:3015.5954\n",
      "Loss:3015.5942\n",
      "Loss:3015.5932\n",
      "Loss:3015.5926\n",
      "Loss:3015.5918\n",
      "Loss:3015.5906\n",
      "Loss:3015.5899\n",
      "Loss:3015.5889\n",
      "Loss:3015.5884\n",
      "Loss:3015.5882\n",
      "Loss:3015.5876\n",
      "Loss:3015.5872\n",
      "Loss:3015.5868\n",
      "Loss:3015.5866\n",
      "Loss:3015.5863\n",
      "Loss:3015.5859\n",
      "Loss:3015.5856\n",
      "Loss:3015.5852\n",
      "Loss:3015.5848\n",
      "Loss:3015.5842\n",
      "Loss:3015.5839\n",
      "Loss:3015.5836\n",
      "Loss:3015.5832\n",
      "Loss:3015.5830\n",
      "Loss:3015.5829\n",
      "Loss:3015.5822\n",
      "Loss:3015.5820\n",
      "Loss:3015.5816\n",
      "Loss:3015.5815\n",
      "Loss:3015.5813\n",
      "Loss:3015.5809\n",
      "Loss:3015.5806\n",
      "Loss:3015.5803\n",
      "Loss:3015.5801\n",
      "Loss:3015.5798\n",
      "Loss:3015.5796\n",
      "Loss:3015.5796\n",
      "Loss:3015.5795\n",
      "Loss:3015.5795\n",
      "Loss:3015.5795\n",
      "Loss:3015.5794\n",
      "Loss:3015.5793\n",
      "Loss:3015.5791\n",
      "Loss:3015.5789\n",
      "Loss:3015.5789\n",
      "Loss:3015.5788\n",
      "Loss:3015.5788\n",
      "Loss:3015.5787\n",
      "Loss:3015.5787\n",
      "Loss:3015.5786\n",
      "Loss:3015.5786\n",
      "Loss:3015.5785\n",
      "Loss:3015.5783\n",
      "Loss:3015.5782\n",
      "Loss:3015.5782\n",
      "Loss:3015.5781\n",
      "Loss:3015.5781\n",
      "Loss:3015.5780\n",
      "Loss:3015.5780\n",
      "Loss:3015.5779\n",
      "Loss:3015.5779\n",
      "Loss:3015.5779\n",
      "Loss:3015.5778\n",
      "Loss:3015.5777\n",
      "Loss:3015.5777\n",
      "Loss:3015.5776\n",
      "Loss:3015.5776\n",
      "Loss:3015.5776\n",
      "Loss:3015.5776\n",
      "Loss:3015.5775\n",
      "Loss:3015.5775\n",
      "Loss:3015.5775\n",
      "Loss:3015.5775\n",
      "Loss:3015.5774\n",
      "Loss:3015.5774\n",
      "Loss:3015.5774\n",
      "Loss:3015.5774\n",
      "Loss:3015.5774\n",
      "Loss:3015.5774\n",
      "Loss:3015.5773\n",
      "Loss:3015.5773\n",
      "Loss:3015.5773\n",
      "Loss:3015.5773\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import fmin_l_bfgs_b \n",
    "\n",
    "states = get_states(data_dir / dataset / \"train\")\n",
    "train_inputs, train_labels = get_dataset(data_dir / dataset / \"train\")\n",
    "\n",
    "def callbackF(w):\n",
    "    '''\n",
    "    This function will be called by \"fmin_l_bfgs_b\"\n",
    "    Arg:\n",
    "        w: weights, numpy array\n",
    "    '''\n",
    "    loss = compute_crf_loss(train_inputs,train_labels,f,states,0.1,regularization=True) \n",
    "    print('Loss:{0:.4f}'.format(loss))\n",
    "\n",
    "def get_loss_grad(w,*args): \n",
    "    '''\n",
    "    This function will be called by \"fmin_l_bfgs_b\"\n",
    "    Arg:\n",
    "        w: weights, numpy array\n",
    "    Returns:\n",
    "        loss: loss, float\n",
    "        grads: gradients, numpy array\n",
    "    '''\n",
    "\n",
    "    train_inputs,train_labels,f,states = args\n",
    "    f = numpy_to_dict(w,f)\n",
    "    # compute loss and grad\n",
    "    loss = compute_crf_loss(train_inputs, train_labels, f, states, 0.1, regularization=True)\n",
    "    grads = compute_gradients(train_inputs, train_labels, f, states, 0.1, regularization=True)\n",
    "    grads = prepare_grad_for_bfgs(grads, f) \n",
    "    # return loss and grad\n",
    "    return loss, grads\n",
    "\n",
    "init_w = np.zeros(len(f))\n",
    "result = fmin_l_bfgs_b(get_loss_grad, init_w, args=(train_inputs, train_labels, f, states), pgtol=0.01, callback=callbackF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = numpy_to_dict(result[0], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference(data_dir / dataset / \"dev.in\", states, f, \"p4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
