{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "data_dir = Path(\"data/\")\n",
    "dataset = \"EN\" # EN or ES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1(i): Emission scores\n",
    "Compute $e(x|y) = \\frac{\\text{Count}(y \\rightarrow x)}{\\text{Count}(y)}$ and store it into a dictionary $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_emission_scores(path):\n",
    "    '''\n",
    "    Inputs:\n",
    "        path (Path object or str): path on the local directory to the dataset to load from.\n",
    "    Outputs:\n",
    "        f (dict): Key-value mapping of str(x, y) to log(e(x|y)) values.\n",
    "    '''\n",
    "\n",
    "    count_emission = defaultdict(int) # Stores Count(y -> x), where key is tuple (x, y), and value is Count(y -> x)\n",
    "    count_labels = defaultdict(int) # Stores Count(y), where key is y\n",
    "    \n",
    "    # Read from dataset path\n",
    "    with open(path) as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "        # Process lines\n",
    "        for line in lines:\n",
    "            # Strip newline\n",
    "            formatted_line = line.strip()\n",
    "            \n",
    "            # Only process lines that are not newlines\n",
    "            if len(formatted_line) > 0:\n",
    "                # Split into (x, y) pair\n",
    "                split_data = formatted_line.split(\" \")\n",
    "                x, y = split_data[0].lower(), split_data[1]\n",
    "\n",
    "                count_emission[(x, y)] += 1\n",
    "                count_labels[y] += 1\n",
    "    \n",
    "    # Result dictionary that maps str(x, y) to log(e(x|y)) values\n",
    "    f = {}\n",
    "    \n",
    "    # Estimate e(x|y) based on counts\n",
    "    for x, y in count_emission:\n",
    "        # Create str(x, y)\n",
    "        feature_str = f\"emission:{y}+{x}\"\n",
    "        e = count_emission[(x, y)] / count_labels[y]\n",
    "        \n",
    "        # Store score\n",
    "        f[feature_str] = np.log(e)\n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1(ii): Transition scores\n",
    "Compute $q(y_i|y_{i-1}) = \\frac{\\text{Count}(y_{i-1}, y_i)}{\\text{Count}(y_{i-1})}$ and store it into a dictionary $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_transition_scores(path):\n",
    "    '''\n",
    "    Inputs:\n",
    "        path (Path object or str): path on the local directory to the dataset to load from.\n",
    "    Outputs:\n",
    "        f (dict): Key-value mapping of str(y_{i-1}, y_i) to log(q(y_i|y_{i-1})) values.\n",
    "    '''\n",
    "    \n",
    "    count_transition = defaultdict(int) # Key is tuple (y_i, y_{i-1}), and value is Count(y_{i-1}, y_i)\n",
    "    count_labels = defaultdict(int) # Stores Count(y_i), where key is y_i\n",
    "    \n",
    "    # Read from dataset path\n",
    "    with open(path) as f:\n",
    "        lines = f.readlines()\n",
    "        # Initialize prev_y as START\n",
    "        prev_y = \"START\"\n",
    "        \n",
    "        # Process lines\n",
    "        for line in lines:\n",
    "            # Strip newline\n",
    "            formatted_line = line.strip()\n",
    "            \n",
    "            # Only process lines that are not newlines\n",
    "            if len(formatted_line) > 0:\n",
    "                # Split into (x, y) pair\n",
    "                split_data = formatted_line.split(\" \")\n",
    "                x, y = split_data[0].lower(), split_data[1]\n",
    "                \n",
    "                transition_key = (prev_y, y)\n",
    "                count_transition[transition_key] += 1\n",
    "                count_labels[y] += 1\n",
    "                \n",
    "                # Update for next word\n",
    "                prev_y = y\n",
    "            else:\n",
    "                # End of sentence\n",
    "                # Store Count(STOP|y_n)\n",
    "                transition_key = (prev_y, \"STOP\")\n",
    "                count_transition[transition_key] += 1\n",
    "                \n",
    "                # Start of next sentence: initialize prev_y to \"START\" and store Count(START)\n",
    "                prev_y = \"START\"\n",
    "                count_labels[prev_y] += 1\n",
    "    \n",
    "    # Result dictionary that maps str(x, y) to log(e(x|y)) values\n",
    "    f = {}\n",
    "    \n",
    "    # Estimate e(x|y) based on counts\n",
    "    for prev_y, y in count_transition:\n",
    "        # Create str(y_{i-1}, y_i)\n",
    "        feature_str = f\"transition:{prev_y}+{y}\"\n",
    "        q = count_transition[(prev_y, y)] / count_labels[prev_y]\n",
    "        \n",
    "        # Store score\n",
    "        f[feature_str] = np.log(q)\n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute emission and transition parameters\n",
    "f_emission_train = calculate_emission_scores(data_dir / dataset / \"train\")\n",
    "f_transition_train = calculate_transition_scores(data_dir / dataset / \"train\")\n",
    "\n",
    "# Combine the transition and emission dictionaries together\n",
    "f = {**f_emission_train, **f_transition_train}\n",
    "# Ensure the number of elements is correct\n",
    "assert(len(f) == len(f_emission_train) + len(f_transition_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2(i)\n",
    "Compute CRF scores for a given input and output sequence pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-44.57667948595218"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_crf_score(x, y, feature_dict):\n",
    "    ''' \n",
    "    Inputs:\n",
    "        x (list[str]): Complete input word sentence (without START or STOP tags)\n",
    "        y (list[str]): Complete output label sequence\n",
    "        feature_dict (dict[str] -> float): Dictionary that maps a given feature to its score.\n",
    "    Outputs:\n",
    "        p (float): Score given by p(y | x)\n",
    "    '''\n",
    "    \n",
    "    # Input and output sequences must be of the same length\n",
    "    assert(len(x) == len(y))\n",
    "    n = len(x) # Sequence length\n",
    "    \n",
    "    # Stores the number of times each feature appears in (x, y)\n",
    "    feature_count = defaultdict(int)\n",
    "    \n",
    "    # Compute emission features\n",
    "    for i in range(n):\n",
    "        formatted_word = x[i].lower()\n",
    "        emission_key = f\"emission:{y[i]}+{formatted_word}\"\n",
    "        feature_count[emission_key] += 1\n",
    "    \n",
    "    # Compute transition features\n",
    "    # Add START and STOP tags to y\n",
    "    updated_y = [\"START\"] + y + [\"STOP\"]\n",
    "    for i in range(1, n+2):\n",
    "        prev_y = updated_y[i-1]\n",
    "        y_i = updated_y[i]\n",
    "        transition_key = f\"transition:{prev_y}+{y_i}\"\n",
    "        feature_count[transition_key] += 1\n",
    "    \n",
    "    # Compute score\n",
    "    score = 0\n",
    "    for feature_key, count in feature_count.items():\n",
    "        weight = feature_dict[feature_key]\n",
    "        score += weight * count\n",
    "    \n",
    "    return score\n",
    "\n",
    "# Test function\n",
    "x = \"Great food with an awesome atmosphere !\".split()\n",
    "y = \"O B-positive O O O B-positive O\".split()\n",
    "compute_crf_score(x, y, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2(ii)\n",
    "Viterbi algorithm for decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-positive', 'O', 'O', 'O', 'B-positive', 'O']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_states(path):\n",
    "    '''\n",
    "    Inputs:\n",
    "        path (Path object or str): path on the local directory to the dataset to load from.\n",
    "    Outputs:\n",
    "        states (list[str]): Unique states in the dataset.\n",
    "    '''\n",
    "    states = set()\n",
    "    \n",
    "    # Read from dataset path\n",
    "    with open(path) as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "        # Process lines\n",
    "        for line in lines:\n",
    "            # Strip newline\n",
    "            formatted_line = line.strip()\n",
    "            \n",
    "            # Only process lines that are not newlines\n",
    "            if len(formatted_line) > 0:\n",
    "                # Split into (x, y) pair\n",
    "                split_data = formatted_line.split(\" \")\n",
    "                x, y = split_data[0], split_data[1]\n",
    "                states.add(y)\n",
    "    \n",
    "    return list(states)\n",
    "\n",
    "def viterbi_decode(x, states, feature_dict, default_index=0):\n",
    "    '''\n",
    "    Inputs:\n",
    "        x (list[str]): Input sequence.\n",
    "        states (list[str]): Possible output states.\n",
    "        feature_dict (dict[str] -> float): Dictionary that maps a given feature to its score.\n",
    "        default_index (int, optional): Index to default for backpointer if scores are nan.\n",
    "    Outputs:\n",
    "        y (list[str]): Most probable output sequence.\n",
    "    '''\n",
    "    \n",
    "    n = len(x) # Number of words\n",
    "    d = len(states) # Number of states\n",
    "    scores = np.full((n, d), np.nan)\n",
    "    bp = np.full((n, d), default_index, dtype=np.int) # TODO: Default to 'O', or something else?\n",
    "\n",
    "    # Convert to lowercase\n",
    "    x = [x[i].lower() for i in range(n)]\n",
    "    \n",
    "    # Compute START transition scores\n",
    "    for i, current_y in enumerate(states):\n",
    "        transition_key = f\"transition:START+{current_y}\"\n",
    "        emission_key = f\"emission:{current_y}+{x[0]}\"\n",
    "        if transition_key in feature_dict and emission_key in feature_dict:\n",
    "            transmission_score = feature_dict[transition_key]\n",
    "            emission_score = feature_dict[emission_key]\n",
    "            scores[0, i] = transmission_score + emission_score\n",
    "    \n",
    "    # Recursively compute best scores based on transmission and emission scores at each node\n",
    "    for i in range(1, n):\n",
    "        for k, prev_y in enumerate(states):\n",
    "            for j, current_y in enumerate(states):\n",
    "                transition_key = f\"transition:{prev_y}+{current_y}\"\n",
    "                emission_key = f\"emission:{current_y}+{x[i]}\"\n",
    "                \n",
    "                # Only consider if the feature exists\n",
    "                if transition_key in feature_dict and emission_key in feature_dict:\n",
    "                    transition_score = feature_dict[transition_key]\n",
    "                    emission_score = feature_dict[emission_key]\n",
    "                    overall_score = emission_score + transition_score + scores[i-1, k]\n",
    "                    \n",
    "                    # Better score is found: Update backpointer and score arrays\n",
    "                    if (np.isnan(scores[i, j]) and not np.isnan(overall_score)) or overall_score > scores[i, j]:\n",
    "                        scores[i, j] = overall_score\n",
    "                        bp[i,j] = k\n",
    "    \n",
    "    # Compute for STOP\n",
    "    highest_score = None\n",
    "    highest_bp = default_index\n",
    "    \n",
    "    for j, prev_y in enumerate(states):\n",
    "        if not np.isnan(scores[n-1, j]):\n",
    "            transition_key = f\"transition:{prev_y}+STOP\"\n",
    "\n",
    "            if transition_key in feature_dict:\n",
    "                transition_score = feature_dict[transition_key]\n",
    "                overall_score = transition_score + scores[n-1, j]\n",
    "                if highest_score == None or overall_score > highest_score:\n",
    "                    highest_score = overall_score\n",
    "                    highest_bp = j\n",
    "    \n",
    "    # Follow backpointers to get output sequence\n",
    "    result = [states[highest_bp]]\n",
    "    prev_bp = highest_bp\n",
    "    for i in range(n-1, 0, -1):\n",
    "        prev_bp = bp[i, prev_bp]\n",
    "        output = states[prev_bp]\n",
    "        # Prepend result to output list\n",
    "        result = [output] + result\n",
    "    \n",
    "    return result\n",
    "\n",
    "states = get_states(data_dir / dataset / \"train\")\n",
    "viterbi_decode(x, states, f) # Should be similar or equal to y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform decoding on dev sets\n",
    "def inference(path, states, feature_dict):\n",
    "    '''\n",
    "    Given a path, perform inference on sentences in the dataset and writes it to disk.\n",
    "    \n",
    "    Inputs:\n",
    "        path (Path object or str): path on the local directory to the dataset to perform inference on.\n",
    "        states (list[str]): Unique states that can be predicted.\n",
    "        feature_dict (dict[str] -> float): Dictionary that maps a given feature to its score.\n",
    "    Outputs:\n",
    "        None\n",
    "    '''\n",
    "    \n",
    "    default_index = states.index('O')\n",
    "    sentences = []\n",
    "\n",
    "    # Write predictions to file\n",
    "    output_filename = str(path).replace(\".in\", \".p2.out\")\n",
    "\n",
    "    # Read from dataset path\n",
    "    with open(path) as f:\n",
    "        lines = f.readlines()\n",
    "        sentence = []\n",
    "        \n",
    "        for line in lines:\n",
    "            formatted_line = line.strip()\n",
    "            \n",
    "            # Not the end of sentence, add it to the list\n",
    "            if len(formatted_line) > 0:\n",
    "                sentence.append(formatted_line)\n",
    "            else:\n",
    "                # End of sentence\n",
    "                sentences.append(sentence)\n",
    "                sentence = []\n",
    "    \n",
    "    # Write output file\n",
    "    with open(output_filename, \"w\") as wf:\n",
    "        for sentence in sentences:\n",
    "            # Run predictions\n",
    "            pred_sentence = viterbi_decode(sentence, states, feature_dict, default_index)\n",
    "            \n",
    "            # Write original word and predicted tags\n",
    "            for i in range(len(sentence)):\n",
    "                wf.write(sentence[i] + \" \" + pred_sentence[i] + \"\\n\")\n",
    "            \n",
    "            # End of sentence, write newline\n",
    "            wf.write(\"\\n\")\n",
    "\n",
    "inference(data_dir / dataset / \"dev.in\", states, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3(i)\n",
    "Loss calculation using forward algorithm. We first define the score calculation functions for a single sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_forward_score(x, feature_dict, states):\n",
    "    '''\n",
    "    Uses the forward algorithm to compute the score for a given sequence.\n",
    "    \n",
    "    Inputs:\n",
    "        x (list[str]): Input sequence.\n",
    "        feature_dict (dict[str] -> float): Dictionary that maps a given feature to its score.\n",
    "        states (list[str]): List of unique states.\n",
    "    Outputs:\n",
    "        forward_score (float): Forward score for this sequence.\n",
    "    '''\n",
    "    \n",
    "    n = len(x) # Number of words\n",
    "    d = len(states) # Number of states\n",
    "    scores = np.full((n, d), np.nan)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    x = [x[i].lower() for i in range(n)]\n",
    "    \n",
    "    # Compute START transition scores\n",
    "    for i, current_y in enumerate(states):\n",
    "        transition_key = f\"transition:START+{current_y}\"\n",
    "        emission_key = f\"emission:{current_y}+{x[0]}\"\n",
    "        if transition_key in feature_dict and emission_key in feature_dict:\n",
    "            transmission_score = feature_dict[transition_key]\n",
    "            emission_score = feature_dict[emission_key]\n",
    "            # Sum exponentials\n",
    "            scores[0, i] = np.exp(transmission_score + emission_score)\n",
    "    \n",
    "    # Recursively compute best scores based on transmission and emission scores at each node\n",
    "    for i in range(1, n):\n",
    "        for k, prev_y in enumerate(states):\n",
    "            for j, current_y in enumerate(states):\n",
    "                transition_key = f\"transition:{prev_y}+{current_y}\"\n",
    "                emission_key = f\"emission:{current_y}+{x[i]}\"\n",
    "                \n",
    "                # Only consider if the feature exists\n",
    "                if transition_key in feature_dict and emission_key in feature_dict:\n",
    "                    transition_score = feature_dict[transition_key]\n",
    "                    emission_score = feature_dict[emission_key]\n",
    "                    \n",
    "                    # Sum exponentials\n",
    "                    overall_score = np.exp(emission_score + transition_score) * scores[i-1, k]\n",
    "                    \n",
    "                    # Better score is found: Add to score array\n",
    "                    if not np.isnan(overall_score):\n",
    "                        if np.isnan(scores[i, j]):\n",
    "                            scores[i, j] = 0\n",
    "                        \n",
    "                        scores[i, j] += overall_score\n",
    "    \n",
    "    # Compute for STOP\n",
    "    forward_score = 0\n",
    "    \n",
    "    for j, prev_y in enumerate(states):\n",
    "        if not np.isnan(scores[n-1, j]):\n",
    "            transition_key = f\"transition:{prev_y}+STOP\"\n",
    "\n",
    "            if transition_key in feature_dict:\n",
    "                transition_score = feature_dict[transition_key]\n",
    "                \n",
    "                # Sum exponentials\n",
    "                overall_score = np.exp(transition_score) * scores[n-1, j]\n",
    "                forward_score += overall_score\n",
    "    \n",
    "    # Take log over entire sum\n",
    "    forward_score = np.log(forward_score)\n",
    "    \n",
    "    return forward_score\n",
    "\n",
    "\n",
    "def compute_crf_loss_sample(x, y, feature_dict, states):\n",
    "    '''\n",
    "    Inputs:\n",
    "        x (list[str]): Input sequence.\n",
    "        y (list[str]): Groundtruth output sequence.\n",
    "        feature_dict (dict[str] -> float): Dictionary that maps a given feature to its score.\n",
    "        states (list[str]): List of unique states.\n",
    "    Outputs:\n",
    "        loss (float): Loss value for a single sample.\n",
    "    '''\n",
    "    first_term = compute_crf_score(x, y, feature_dict)\n",
    "    forward_score = compute_forward_score(x, feature_dict, states)\n",
    "    \n",
    "    loss = -(first_term - forward_score)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the methods defined early to compute the loss across the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2178.0848344696287"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_dataset(path):\n",
    "    '''\n",
    "    Given a path, load the data from file.\n",
    "    \n",
    "    Inputs:\n",
    "        path (Path object or str): path on the local directory to the dataset to perform inference on.\n",
    "    Outputs:\n",
    "        input_sequences (list[list[str]]): List of input sequences\n",
    "        input_labels (list[list[str]]): List of input labels\n",
    "    '''\n",
    "    input_sequences = []\n",
    "    input_labels = []\n",
    "    \n",
    "    # Read from dataset path\n",
    "    with open(path) as f:\n",
    "        lines = f.readlines()\n",
    "        sentence = []\n",
    "        labels = []\n",
    "        \n",
    "        for line in lines:\n",
    "            formatted_line = line.strip()\n",
    "            \n",
    "            # Not the end of sentence, add it to the list\n",
    "            if len(formatted_line) > 0:\n",
    "                split_data = formatted_line.split(\" \")\n",
    "                x, y = split_data[0].lower(), split_data[1]\n",
    "\n",
    "                sentence.append(x)\n",
    "                labels.append(y)\n",
    "            else:\n",
    "                # End of sentence\n",
    "                input_sequences.append(sentence)\n",
    "                input_labels.append(labels)\n",
    "                sentence = []\n",
    "                labels = []\n",
    "    \n",
    "    return input_sequences, input_labels\n",
    "\n",
    "def compute_crf_loss(input_sequences, input_labels, feature_dict, states):\n",
    "    '''\n",
    "    Given a path, perform inference on sentences in the dataset and writes it to disk.\n",
    "    \n",
    "    Inputs:\n",
    "        input_sequences (list[list[str]]): List of input sequences\n",
    "        input_labels (list[list[str]]): List of input labels\n",
    "        feature_dict (dict[str] -> float): Dictionary that maps a given feature to its score.\n",
    "        states (list[str]): List of unique states.\n",
    "    Outputs:\n",
    "        loss (float): Loss value for the dataset.\n",
    "    '''\n",
    "    loss = 0\n",
    "    for i in range(len(input_sequences)):\n",
    "        sample_loss = compute_crf_loss_sample(input_sequences[i], input_labels[i], feature_dict, states)\n",
    "        loss += sample_loss\n",
    "    \n",
    "    return loss\n",
    "\n",
    "f_zero = {k: -3 for k in f}\n",
    "train_inputs, train_labels = get_dataset(data_dir / dataset / \"train\")\n",
    "compute_crf_loss(train_inputs, train_labels, f, states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float,\n",
       "            {'emission:B-negative+(': 0.0,\n",
       "             'emission:B-negative+)': 0.0,\n",
       "             'emission:B-negative+,': 0.0,\n",
       "             'emission:B-negative+.': 0.0,\n",
       "             'emission:B-negative+all': 0.02260174720026268,\n",
       "             'emission:B-negative+dessserts': 1.0000000000000002,\n",
       "             'emission:B-negative+except': 0.0,\n",
       "             'emission:B-negative+food': 0.16207416808999195,\n",
       "             'emission:B-negative+for': 0.0,\n",
       "             'emission:B-negative+great': 0.0,\n",
       "             'emission:B-negative+in': 0.0,\n",
       "             'emission:B-negative+the': 0.00027086185197281094,\n",
       "             'emission:B-negative+was': 0.0,\n",
       "             'emission:B-neutral+(': 0.0,\n",
       "             'emission:B-neutral+)': 0.0,\n",
       "             'emission:B-neutral+,': 0.0,\n",
       "             'emission:B-neutral+.': 0.0,\n",
       "             'emission:B-neutral+all': 0.0,\n",
       "             'emission:B-neutral+dessserts': 0.0,\n",
       "             'emission:B-neutral+except': 0.0,\n",
       "             'emission:B-neutral+food': 0.0625638096036594,\n",
       "             'emission:B-neutral+for': 0.0,\n",
       "             'emission:B-neutral+great': 0.0,\n",
       "             'emission:B-neutral+in': 0.0,\n",
       "             'emission:B-neutral+the': 0.0004927559528001558,\n",
       "             'emission:B-neutral+was': 0.0,\n",
       "             'emission:B-positive+(': 0.0,\n",
       "             'emission:B-positive+)': 0.0,\n",
       "             'emission:B-positive+,': 0.0,\n",
       "             'emission:B-positive+.': 0.0,\n",
       "             'emission:B-positive+all': 0.02084101041309877,\n",
       "             'emission:B-positive+dessserts': 0.0,\n",
       "             'emission:B-positive+except': 0.0,\n",
       "             'emission:B-positive+food': 0.5338646792074748,\n",
       "             'emission:B-positive+for': 0.0,\n",
       "             'emission:B-positive+great': 0.0,\n",
       "             'emission:B-positive+in': 0.0,\n",
       "             'emission:B-positive+the': 0.0009627112712188014,\n",
       "             'emission:B-positive+was': 0.0,\n",
       "             'emission:I-negative+(': 0.0,\n",
       "             'emission:I-negative+)': 0.0,\n",
       "             'emission:I-negative+,': 0.0,\n",
       "             'emission:I-negative+.': 0.0,\n",
       "             'emission:I-negative+all': 0.0,\n",
       "             'emission:I-negative+dessserts': 0.0,\n",
       "             'emission:I-negative+except': 0.0,\n",
       "             'emission:I-negative+food': 0.00010408445249352744,\n",
       "             'emission:I-negative+for': 0.0,\n",
       "             'emission:I-negative+great': 0.0,\n",
       "             'emission:I-negative+in': 0.002019256391482528,\n",
       "             'emission:I-negative+the': 0.0,\n",
       "             'emission:I-negative+was': 0.0,\n",
       "             'emission:I-neutral+(': 0.0,\n",
       "             'emission:I-neutral+)': 0.0,\n",
       "             'emission:I-neutral+,': 0.0,\n",
       "             'emission:I-neutral+.': 0.0,\n",
       "             'emission:I-neutral+all': 0.0,\n",
       "             'emission:I-neutral+dessserts': 0.0,\n",
       "             'emission:I-neutral+except': 0.0,\n",
       "             'emission:I-neutral+food': 0.000343878707725927,\n",
       "             'emission:I-neutral+for': 0.0,\n",
       "             'emission:I-neutral+great': 0.0,\n",
       "             'emission:I-neutral+in': 0.0,\n",
       "             'emission:I-neutral+the': 0.0,\n",
       "             'emission:I-neutral+was': 0.0,\n",
       "             'emission:I-positive+(': 0.0,\n",
       "             'emission:I-positive+)': 0.0,\n",
       "             'emission:I-positive+,': 0.0004615484028919783,\n",
       "             'emission:I-positive+.': 0.0,\n",
       "             'emission:I-positive+all': 0.0,\n",
       "             'emission:I-positive+dessserts': 0.0,\n",
       "             'emission:I-positive+except': 0.0,\n",
       "             'emission:I-positive+food': 0.0008735263831795397,\n",
       "             'emission:I-positive+for': 0.0,\n",
       "             'emission:I-positive+great': 0.0,\n",
       "             'emission:I-positive+in': 0.0003885113649234719,\n",
       "             'emission:I-positive+the': 7.002564461129804e-05,\n",
       "             'emission:I-positive+was': 0.0,\n",
       "             'emission:O+(': 1.0,\n",
       "             'emission:O+)': 1.0,\n",
       "             'emission:O+,': 0.9995384515971079,\n",
       "             'emission:O+.': 1.0,\n",
       "             'emission:O+all': 1.956557242386638,\n",
       "             'emission:O+dessserts': 0.0,\n",
       "             'emission:O+except': 1.0000000000000002,\n",
       "             'emission:O+food': 0.24017585355547486,\n",
       "             'emission:O+for': 1.0000000000000002,\n",
       "             'emission:O+great': 1.0,\n",
       "             'emission:O+in': 0.9975922322435936,\n",
       "             'emission:O+the': 1.9982036452793972,\n",
       "             'emission:O+was': 1.0,\n",
       "             'transition:B-negative+I-negative': 0.0028261114245159884,\n",
       "             'transition:B-negative+O': 0.016030811299687833,\n",
       "             'transition:B-negative+STOP': 0.0,\n",
       "             'transition:B-neutral+I-neutral': 0.001443043820623956,\n",
       "             'transition:B-neutral+O': 0.011237771488373774,\n",
       "             'transition:B-neutral+STOP': 0.0,\n",
       "             'transition:B-positive+B-positive': 5.2354026582679357e-05,\n",
       "             'transition:B-positive+I-positive': 0.014842098792223568,\n",
       "             'transition:B-positive+O': 0.05401371535885889,\n",
       "             'transition:B-positive+STOP': 0.0,\n",
       "             'transition:I-negative+I-negative': 1.3098918656357366e-05,\n",
       "             'transition:I-negative+O': 2.8819855508618194e-05,\n",
       "             'transition:I-negative+STOP': 0.0,\n",
       "             'transition:I-neutral+I-neutral': 6.602471188337798e-06,\n",
       "             'transition:I-neutral+O': 1.1875580090435682e-05,\n",
       "             'transition:I-neutral+STOP': 0.0,\n",
       "             'transition:I-positive+I-positive': 1.7590838784122792e-05,\n",
       "             'transition:I-positive+O': 3.660822923710579e-05,\n",
       "             'transition:I-positive+STOP': 0.0,\n",
       "             'transition:O+B-negative': 0.0017668711662256914,\n",
       "             'transition:O+B-neutral': 0.00043862499859033453,\n",
       "             'transition:O+B-positive': 0.008127518744378783,\n",
       "             'transition:O+O': 0.2407272159860541,\n",
       "             'transition:O+STOP': 1.0,\n",
       "             'transition:START+B-negative': 0.010362188060035062,\n",
       "             'transition:START+B-neutral': 0.0,\n",
       "             'transition:START+B-positive': 0.009724526694190787,\n",
       "             'transition:START+I-negative': 0.0,\n",
       "             'transition:START+I-neutral': 0.0,\n",
       "             'transition:START+I-positive': 0.0,\n",
       "             'transition:START+O': 0.9799132852457739})"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def forward_backward(x, feature_dict, states):\n",
    "    '''\n",
    "    Uses the backward algorithm to compute the score for a given sequence.\n",
    "    \n",
    "    Inputs:\n",
    "        x (list[str]): Input sequence.\n",
    "        feature_dict (dict[str] -> float): Dictionary that maps a given feature to its score.\n",
    "        states (list[str]): List of unique states.\n",
    "    Outputs:\n",
    "        backward_score (float): Backward score for this sequence.\n",
    "    '''\n",
    "    \n",
    "    n = len(x) # Number of words\n",
    "    d = len(states) # Number of states\n",
    "    forward_scores = np.zeros((n, d))\n",
    "    backward_scores = np.zeros((n, d))\n",
    "\n",
    "    # Convert to lowercase\n",
    "    x = [x[i].lower() for i in range(n)]\n",
    "\n",
    "    # Start forward pass\n",
    "    # Compute START transition scores\n",
    "    for i, current_y in enumerate(states):\n",
    "        transition_key = f\"transition:START+{current_y}\"\n",
    "        emission_key = f\"emission:{current_y}+{x[0]}\"\n",
    "        if transition_key in feature_dict and emission_key in feature_dict:\n",
    "            transition_score = feature_dict[transition_key]\n",
    "            emission_score = feature_dict[emission_key]\n",
    "            # Sum exponentials\n",
    "            forward_scores[0, i] = np.exp(transition_score + emission_score)\n",
    "    \n",
    "    # Recursively compute best scores based on transmission and emission scores at each node\n",
    "    for i in range(1, n):\n",
    "        for k, prev_y in enumerate(states):\n",
    "            for j, current_y in enumerate(states):\n",
    "                transition_key = f\"transition:{prev_y}+{current_y}\"\n",
    "                emission_key = f\"emission:{current_y}+{x[i]}\"\n",
    "                \n",
    "                # Only consider if the feature exists\n",
    "                if transition_key in feature_dict and emission_key in feature_dict:\n",
    "                    transition_score = feature_dict[transition_key]\n",
    "                    emission_score = feature_dict[emission_key]\n",
    "                    # Sum exponentials\n",
    "                    overall_score = np.exp(emission_score + transition_score) * forward_scores[i-1, k]\n",
    "\n",
    "                    # Better score is found: Add to score array\n",
    "                    forward_scores[i, j] += overall_score\n",
    "\n",
    "    # Compute for STOP\n",
    "    forward_prob = 0\n",
    "    for j, prev_y in enumerate(states):\n",
    "        transition_key = f\"transition:{prev_y}+STOP\"\n",
    "        \n",
    "        if transition_key in feature_dict:\n",
    "            transition_score = feature_dict[transition_key]\n",
    "            # Sum exponentials\n",
    "            overall_score = np.exp(transition_score) * forward_scores[n-1, j]\n",
    "            forward_prob += overall_score\n",
    "    # End forward pass\n",
    "\n",
    "    # Start backward pass\n",
    "    # Compute STOP transition scores\n",
    "    for i, current_y in enumerate(states):\n",
    "        transition_key = f\"transition:{current_y}+STOP\"\n",
    "        if transition_key in feature_dict: # and emission_key in feature_dict:\n",
    "            transition_score = feature_dict[transition_key]\n",
    "            # Sum exponentials\n",
    "            backward_scores[n-1, i] = np.exp(transition_score)\n",
    "    \n",
    "    # Recursively compute best scores based on transmission and emission scores at each node\n",
    "    for i in range(n-1, 0, -1):\n",
    "        for k, next_y in enumerate(states):\n",
    "            for j, current_y in enumerate(states):\n",
    "                transition_key = f\"transition:{current_y}+{next_y}\"\n",
    "                emission_key = f\"emission:{next_y}+{x[i]}\"\n",
    "                \n",
    "                # Only consider if the feature exists\n",
    "                if transition_key in feature_dict and emission_key in feature_dict:\n",
    "                    transition_score = feature_dict[transition_key]\n",
    "                    emission_score = feature_dict[emission_key]\n",
    "                    \n",
    "                    # Sum exponentials\n",
    "                    overall_score = np.exp(emission_score + transition_score) * backward_scores[i, k]\n",
    "                    \n",
    "                    # Better score is found: Add to score array\n",
    "                    backward_scores[i-1, j] += overall_score\n",
    "    \n",
    "    # Compute for START\n",
    "    backward_prob = 0\n",
    "    \n",
    "    for j, next_y in enumerate(states):\n",
    "        transition_key = f\"transition:START+{next_y}\"\n",
    "        emission_key = f\"emission:{next_y}+{x[0]}\" # Emission of last word\n",
    "\n",
    "        if transition_key in feature_dict and emission_key in feature_dict:\n",
    "            transition_score = feature_dict[transition_key]\n",
    "            emission_score = feature_dict[emission_key]\n",
    "            # Sum exponentials\n",
    "            overall_score = np.exp(emission_score + transition_score) * backward_scores[0, j]\n",
    "            backward_prob += overall_score\n",
    "    # End backward pass\n",
    "    \n",
    "    # Ensure forward and backward prob are the same up to floating point errors\n",
    "    assert(abs(forward_prob - backward_prob) < 1e-8)\n",
    "\n",
    "    expected_counts = np.zeros((n, d))\n",
    "    for i in range(n):\n",
    "        for j, y in enumerate(states):\n",
    "            expected_counts[i, j] = forward_scores[i, j] * backward_scores[i, j] / forward_prob\n",
    "    \n",
    "    feature_expected_counts = defaultdict(float)\n",
    "    \n",
    "    # Compute expected emission counts\n",
    "    for i in range(n):\n",
    "        for j, current_y in enumerate(states):\n",
    "            emission_key = f\"emission:{current_y}+{x[i]}\"\n",
    "            feature_expected_counts[emission_key] += expected_counts[i, j]\n",
    "    \n",
    "    # Compute expected START / STOP transition counts\n",
    "    for j, next_y in enumerate(states):\n",
    "        start_transition_key = f\"transition:START+{next_y}\"\n",
    "        feature_expected_counts[start_transition_key] += expected_counts[0, j]\n",
    "        \n",
    "        stop_transition_key = f\"transition:{next_y}+STOP\"\n",
    "        feature_expected_counts[stop_transition_key] += expected_counts[n-1, j]\n",
    "\n",
    "    # Compute expected transition counts\n",
    "    for i in range(0, n-1):\n",
    "        for j, current_y in enumerate(states):\n",
    "            for k, next_y in enumerate(states):\n",
    "                transition_key = f\"transition:{current_y}+{next_y}\"\n",
    "#                 feature_expected_counts[transition_key] += expected_counts[i, j] * expected_counts[i+1, k]              \n",
    "                emission_key = f\"emission:{current_y}+{x[i]}\"\n",
    "    \n",
    "                if transition_key in feature_dict and emission_key in feature_dict:\n",
    "                    transition_score = feature_dict[transition_key]\n",
    "                    emission_score = feature_dict[emission_key]\n",
    "                    overall_score = np.exp(transition_score + emission_score)\n",
    "                    \n",
    "                    denom = np.sum(forward_scores[i,:] * backward_scores[i,:])\n",
    "                    transition_expected_counts = forward_scores[i, j] * backward_scores[i, k] * overall_score / denom\n",
    "                    \n",
    "                    assert(transition_expected_counts <= 1)\n",
    "\n",
    "                    feature_expected_counts[transition_key] += transition_expected_counts\n",
    "    \n",
    "    return feature_expected_counts\n",
    "\n",
    "forward_backward(['all', 'in', 'all', ',', 'the', 'food', 'was', 'great', '(', 'except', 'for', 'the', 'dessserts', ')', '.'], f, states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_count(x, y, feature_dict):\n",
    "    ''' \n",
    "    Inputs:\n",
    "        x (list[str]): Complete input word sentence (without START or STOP tags)\n",
    "        y (list[str]): Complete output label sequence\n",
    "        feature_dict (dict[str] -> float): Dictionary that maps a given feature to its score.\n",
    "    Outputs:\n",
    "        p (float): Score given by p(y | x)\n",
    "    '''\n",
    "    \n",
    "    # Input and output sequences must be of the same length\n",
    "    assert(len(x) == len(y))\n",
    "    n = len(x) # Sequence length\n",
    "    \n",
    "    # Stores the number of times each feature appears in (x, y)\n",
    "    feature_count = defaultdict(int)\n",
    "    \n",
    "    # Compute emission features\n",
    "    for i in range(n):\n",
    "        formatted_word = x[i].lower()\n",
    "        emission_key = f\"emission:{y[i]}+{formatted_word}\"\n",
    "        feature_count[emission_key] += 1\n",
    "    \n",
    "    # Compute transition features\n",
    "    # Add START and STOP tags to y\n",
    "    updated_y = [\"START\"] + y + [\"STOP\"]\n",
    "    for i in range(1, n+2):\n",
    "        prev_y = updated_y[i-1]\n",
    "        y_i = updated_y[i]\n",
    "        transition_key = f\"transition:{prev_y}+{y_i}\"\n",
    "        feature_count[transition_key] += 1\n",
    "    \n",
    "    return feature_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_gradients = defaultdict(float)\n",
    "limit = 3\n",
    "\n",
    "for i in range(limit):#len(train_labels)):\n",
    "    x = train_inputs[i]\n",
    "    y = train_labels[i]\n",
    "    \n",
    "    feature_expected_counts = forward_backward(x, f, states)\n",
    "    actual_counts = get_feature_count(x, y, f)\n",
    "\n",
    "    for k, v in feature_expected_counts.items():\n",
    "        feature_gradients[k] += v\n",
    "\n",
    "    for k, v in actual_counts.items():\n",
    "        feature_gradients[k] -= v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6420607405743795 -17.710934402902545\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-291-4be2d42d7ce1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Ensure numerical and analytical gradient are close\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manalytical_gradient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumerical_gradient\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0manalytical_gradient\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Check against numerical gradient is equal\n",
    "# feature_key = 'emission:B-positive+food'\n",
    "feature_key = 'transition:O+O'\n",
    "new_f = f.copy()\n",
    "delta = 1e-6\n",
    "\n",
    "loss1 = compute_crf_loss(train_inputs[:limit], train_labels[:limit], f, states)\n",
    "new_f[feature_key] += delta\n",
    "loss2 = compute_crf_loss(train_inputs[:limit], train_labels[:limit], new_f, states)\n",
    "\n",
    "numerical_gradient = (loss2 - loss1) / delta\n",
    "analytical_gradient = feature_gradients[feature_key]\n",
    "# Ensure numerical and analytical gradient are close\n",
    "print(numerical_gradient, analytical_gradient)\n",
    "assert(abs(numerical_gradient - analytical_gradient) <= 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
