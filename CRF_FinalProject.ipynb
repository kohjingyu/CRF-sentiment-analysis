{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "data_dir = Path(\"data/\")\n",
    "dataset = \"EN\" # EN or ES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1(i): Emission scores\n",
    "Compute $e(x|y) = \\frac{\\text{Count}(y \\rightarrow x)}{\\text{Count}(y)}$ and store it into a dictionary $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_emission_scores(path):\n",
    "    '''\n",
    "    Inputs:\n",
    "        path (Path object or str): path on the local directory to the dataset to load from.\n",
    "    Outputs:\n",
    "        f (dict): Key-value mapping of str(x, y) to log(e(x|y)) values.\n",
    "    '''\n",
    "\n",
    "    count_emission = defaultdict(int) # Stores Count(y -> x), where key is tuple (x, y), and value is Count(y -> x)\n",
    "    count_labels = defaultdict(int) # Stores Count(y), where key is y\n",
    "    \n",
    "    # Read from dataset path\n",
    "    with open(path) as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "        # Process lines\n",
    "        for line in lines:\n",
    "            # Strip newline\n",
    "            formatted_line = line.strip()\n",
    "            \n",
    "            # Only process lines that are not newlines\n",
    "            if len(formatted_line) > 0:\n",
    "                # Split into (x, y) pair\n",
    "                split_data = formatted_line.split(\" \")\n",
    "                x, y = split_data[0].lower(), split_data[1]\n",
    "\n",
    "                count_emission[(x, y)] += 1\n",
    "                count_labels[y] += 1\n",
    "    \n",
    "    # Result dictionary that maps str(x, y) to log(e(x|y)) values\n",
    "    f = {}\n",
    "    \n",
    "    # Estimate e(x|y) based on counts\n",
    "    for x, y in count_emission:\n",
    "        # Create str(x, y)\n",
    "        feature_str = f\"emission:{y}+{x}\"\n",
    "        e = count_emission[(x, y)] / count_labels[y]\n",
    "        \n",
    "        # Store score\n",
    "        f[feature_str] = np.log(e)\n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1(ii): Transition scores\n",
    "Compute $q(y_i|y_{i-1}) = \\frac{\\text{Count}(y_{i-1}, y_i)}{\\text{Count}(y_{i-1})}$ and store it into a dictionary $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_transition_scores(path):\n",
    "    '''\n",
    "    Inputs:\n",
    "        path (Path object or str): path on the local directory to the dataset to load from.\n",
    "    Outputs:\n",
    "        f (dict): Key-value mapping of str(y_{i-1}, y_i) to log(q(y_i|y_{i-1})) values.\n",
    "    '''\n",
    "    \n",
    "    count_transition = defaultdict(int) # Key is tuple (y_i, y_{i-1}), and value is Count(y_{i-1}, y_i)\n",
    "    count_labels = defaultdict(int) # Stores Count(y_i), where key is y_i\n",
    "    \n",
    "    # Read from dataset path\n",
    "    with open(path) as f:\n",
    "        lines = f.readlines()\n",
    "        # Initialize prev_y as START\n",
    "        prev_y = \"START\"\n",
    "        \n",
    "        # Process lines\n",
    "        for line in lines:\n",
    "            # Strip newline\n",
    "            formatted_line = line.strip()\n",
    "            \n",
    "            # Only process lines that are not newlines\n",
    "            if len(formatted_line) > 0:\n",
    "                # Split into (x, y) pair\n",
    "                split_data = formatted_line.split(\" \")\n",
    "                x, y = split_data[0].lower(), split_data[1]\n",
    "                \n",
    "                transition_key = (prev_y, y)\n",
    "                count_transition[transition_key] += 1\n",
    "                count_labels[y] += 1\n",
    "                \n",
    "                # Update for next word\n",
    "                prev_y = y\n",
    "            else:\n",
    "                # End of sentence\n",
    "                # Store Count(STOP|y_n)\n",
    "                transition_key = (prev_y, \"STOP\")\n",
    "                count_transition[transition_key] += 1\n",
    "                \n",
    "                # Start of next sentence: initialize prev_y to \"START\" and store Count(START)\n",
    "                prev_y = \"START\"\n",
    "                count_labels[prev_y] += 1\n",
    "    \n",
    "    # Result dictionary that maps str(x, y) to log(e(x|y)) values\n",
    "    f = {}\n",
    "    \n",
    "    # Estimate e(x|y) based on counts\n",
    "    for prev_y, y in count_transition:\n",
    "        # Create str(y_{i-1}, y_i)\n",
    "        feature_str = f\"transition:{prev_y}+{y}\"\n",
    "        q = count_transition[(prev_y, y)] / count_labels[prev_y]\n",
    "        \n",
    "        # Store score\n",
    "        f[feature_str] = np.log(q)\n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute emission and transition parameters\n",
    "f_emission_train = calculate_emission_scores(data_dir / dataset / \"train\")\n",
    "f_transition_train = calculate_transition_scores(data_dir / dataset / \"train\")\n",
    "\n",
    "# Combine the transition and emission dictionaries together\n",
    "f = {**f_emission_train, **f_transition_train}\n",
    "# Ensure the number of elements is correct\n",
    "assert(len(f) == len(f_emission_train) + len(f_transition_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2(i)\n",
    "Compute CRF scores for a given input and output sequence pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-44.57667948595218"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_crf_score(x, y, feature_dict):\n",
    "    ''' \n",
    "    Inputs:\n",
    "        x (list[str]): Complete input word sentence (without START or STOP tags)\n",
    "        y (list[str]): Complete output label sequence\n",
    "        feature_dict (dict[str] -> float): Dictionary that maps a given feature to its score.\n",
    "    Outputs:\n",
    "        p (float): Score given by p(y | x)\n",
    "    '''\n",
    "    \n",
    "    # Input and output sequences must be of the same length\n",
    "    assert(len(x) == len(y))\n",
    "    n = len(x) # Sequence length\n",
    "    \n",
    "    # Stores the number of times each feature appears in (x, y)\n",
    "    feature_count = defaultdict(int)\n",
    "    \n",
    "    # Compute emission features\n",
    "    for i in range(n):\n",
    "        formatted_word = x[i].lower()\n",
    "        emission_key = f\"emission:{y[i]}+{formatted_word}\"\n",
    "        feature_count[emission_key] += 1\n",
    "    \n",
    "    # Compute transition features\n",
    "    # Add START and STOP tags to y\n",
    "    updated_y = [\"START\"] + y + [\"STOP\"]\n",
    "    for i in range(1, n+2):\n",
    "        prev_y = updated_y[i-1]\n",
    "        y_i = updated_y[i]\n",
    "        transition_key = f\"transition:{prev_y}+{y_i}\"\n",
    "        feature_count[transition_key] += 1\n",
    "    \n",
    "    # Compute score\n",
    "    score = 0\n",
    "    for feature_key, count in feature_count.items():\n",
    "        weight = feature_dict[feature_key]\n",
    "        score += weight * count\n",
    "    \n",
    "    return score\n",
    "\n",
    "x = \"Great food with an awesome atmosphere !\".split()\n",
    "y = \"O B-positive O O O B-positive O\".split()\n",
    "compute_crf_score(x, y, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2(ii)\n",
    "Viterbi algorithm for decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-positive', 'O', 'O', 'O', 'B-positive', 'O']"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_states(path):\n",
    "    '''\n",
    "    Inputs:\n",
    "        path (Path object or str): path on the local directory to the dataset to load from.\n",
    "    Outputs:\n",
    "        states (list[str]): Unique states in the dataset.\n",
    "    '''\n",
    "    states = set()\n",
    "    \n",
    "    # Read from dataset path\n",
    "    with open(path) as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "        # Process lines\n",
    "        for line in lines:\n",
    "            # Strip newline\n",
    "            formatted_line = line.strip()\n",
    "            \n",
    "            # Only process lines that are not newlines\n",
    "            if len(formatted_line) > 0:\n",
    "                # Split into (x, y) pair\n",
    "                split_data = formatted_line.split(\" \")\n",
    "                x, y = split_data[0], split_data[1]\n",
    "                states.add(y)\n",
    "    \n",
    "    return list(states)\n",
    "\n",
    "def viterbi_decode(x, states, feature_dict):\n",
    "    '''\n",
    "    Inputs:\n",
    "        x (list[str]): Input sequence.\n",
    "        states (list[str]): Possible output states.\n",
    "        feature_dict (dict[str] -> float): Dictionary that maps a given feature to its score.\n",
    "    Outputs:\n",
    "        y (list[str]): Most probable output sequence.\n",
    "    '''\n",
    "    \n",
    "    n = len(x) # Number of words\n",
    "    d = len(states) # Number of states\n",
    "    scores = np.full((n, d), np.nan)\n",
    "    bp = np.ones((n, d), dtype=np.int) # TODO: Default to 'O', or something else?\n",
    "\n",
    "    # Convert to lowercase\n",
    "    x = [x[i].lower() for i in range(n)]\n",
    "    \n",
    "    # Compute START transition scores\n",
    "    for i, current_y in enumerate(states):\n",
    "        transition_key = f\"transition:START+{current_y}\"\n",
    "        emission_key = f\"emission:{current_y}+{x[0]}\"\n",
    "        if transition_key in feature_dict and emission_key in feature_dict:\n",
    "            transmission_score = feature_dict[transition_key]\n",
    "            emission_score = feature_dict[emission_key]\n",
    "            scores[0, i] = transmission_score + emission_score\n",
    "    \n",
    "    # Recursively compute best scores based on transmission and emission scores at each node\n",
    "    for i in range(1, n):\n",
    "        for k, prev_y in enumerate(states):\n",
    "            for j, current_y in enumerate(states):\n",
    "                transition_key = f\"transition:{prev_y}+{current_y}\"\n",
    "                emission_key = f\"emission:{current_y}+{x[i]}\"\n",
    "                \n",
    "                # Only consider if the feature exists\n",
    "                if transition_key in feature_dict and emission_key in feature_dict:\n",
    "                    transition_score = feature_dict[transition_key]\n",
    "                    emission_score = feature_dict[emission_key]\n",
    "                    overall_score = emission_score + transition_score + scores[i-1, k]\n",
    "                    \n",
    "                    # Better score is found: Update backpointer and score arrays\n",
    "                    if (np.isnan(scores[i, j]) and not np.isnan(overall_score)) or overall_score > scores[i, j]:\n",
    "                        scores[i, j] = overall_score\n",
    "                        bp[i,j] = k\n",
    "    \n",
    "    # Compute for STOP\n",
    "    highest_score = None\n",
    "    highest_bp = 1\n",
    "    \n",
    "    for j, prev_y in enumerate(states):\n",
    "        if not np.isnan(scores[n-1, j]):\n",
    "            transition_key = f\"transition:{prev_y}+STOP\"\n",
    "\n",
    "            if transition_key in feature_dict:\n",
    "                transition_score = feature_dict[transition_key]\n",
    "                overall_score = transition_score + scores[n-1, j]\n",
    "                if highest_score == None or overall_score > highest_score:\n",
    "                    highest_score = overall_score\n",
    "                    highest_bp = j\n",
    "    \n",
    "    # Follow backpointers to get output sequence\n",
    "    result = [states[highest_bp]]\n",
    "    prev_bp = highest_bp\n",
    "    for i in range(n-1, 0, -1):\n",
    "        prev_bp = bp[i, prev_bp]\n",
    "        output = states[prev_bp]\n",
    "        # Prepend result to output list\n",
    "        result = [output] + result\n",
    "    \n",
    "    return result\n",
    "\n",
    "states = get_states(data_dir / dataset / \"train\")\n",
    "viterbi_decode(x, states, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform decoding on dev sets\n",
    "def inference(path, states, feature_dict):\n",
    "    '''\n",
    "    Given a path, perform inference on sentences in the dataset and writes it to disk.\n",
    "    \n",
    "    Inputs:\n",
    "        path (Path object or str): path on the local directory to the dataset to perform inference on.\n",
    "        states (list[str]): Unique states that can be predicted.\n",
    "        feature_dict (dict[str] -> float): Dictionary that maps a given feature to its score.\n",
    "    Outputs:\n",
    "        None\n",
    "    '''\n",
    "    \n",
    "    # TODO: Can downcase everything? Ask the TA\n",
    "    sentences = []\n",
    "\n",
    "    # Write predictions to file\n",
    "    output_filename = str(path).replace(\".in\", \".p2.out\")\n",
    "\n",
    "    # Read from dataset path\n",
    "    with open(path) as f:\n",
    "        lines = f.readlines()\n",
    "        sentence = []\n",
    "        \n",
    "        for line in lines:\n",
    "            formatted_line = line.strip()\n",
    "            \n",
    "            # Not the end of sentence, add it to the list\n",
    "            if len(formatted_line) > 0:\n",
    "                sentence.append(formatted_line)\n",
    "            else:\n",
    "                # End of sentence\n",
    "                sentences.append(sentence)\n",
    "                sentence = []\n",
    "    \n",
    "    # Write output file\n",
    "    with open(output_filename, \"w\") as wf:\n",
    "        for sentence in sentences:\n",
    "            # Run predictions\n",
    "            pred_sentence = viterbi_decode(sentence, states, feature_dict)\n",
    "            \n",
    "            # Write original word and predicted tags\n",
    "            for i in range(len(sentence)):\n",
    "                wf.write(sentence[i] + \" \" + pred_sentence[i] + \"\\n\")\n",
    "            \n",
    "            # End of sentence, write newline\n",
    "            wf.write(\"\\n\")\n",
    "\n",
    "inference(data_dir / dataset / \"dev.in\", states, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
