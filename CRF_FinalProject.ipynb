{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "data_dir = Path(\"data/\")\n",
    "dataset = \"EN\" # EN or ES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1(i): Emission scores\n",
    "Compute $e(x|y) = \\frac{\\text{Count}(y \\rightarrow x)}{\\text{Count}(y)}$ and store it into a dictionary $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_emission_scores(path):\n",
    "    '''\n",
    "    Inputs:\n",
    "        path (Path object or str): path on the local directory to the dataset to load from.\n",
    "    Outputs:\n",
    "        f (dict): Key-value mapping of str(x, y) to log(e(x|y)) values.\n",
    "    '''\n",
    "\n",
    "    count_emission = defaultdict(int) # Stores Count(y -> x), where key is tuple (x, y), and value is Count(y -> x)\n",
    "    count_labels = defaultdict(int) # Stores Count(y), where key is y\n",
    "    \n",
    "    # For computing pairs that are not aligned\n",
    "    unique_x = set()\n",
    "    unique_y = set()\n",
    "    \n",
    "    # Read from dataset path\n",
    "    with open(path) as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "        # Process lines\n",
    "        for line in lines:\n",
    "            # Strip newline\n",
    "            formatted_line = line.strip()\n",
    "            \n",
    "            # Only process lines that are not newlines\n",
    "            if len(formatted_line) > 0:\n",
    "                # Split into (x, y) pair\n",
    "                split_data = formatted_line.split(\" \")\n",
    "                x, y = split_data[0], split_data[1]\n",
    "\n",
    "                count_emission[(x, y)] += 1\n",
    "                count_labels[y] += 1\n",
    "                \n",
    "                unique_x.add(x)\n",
    "                unique_y.add(y)\n",
    "    \n",
    "    # Result dictionary that maps str(x, y) to log(e(x|y)) values\n",
    "    f = {}\n",
    "    \n",
    "    # Estimate e(x|y) based on counts\n",
    "    for x, y in count_emission:\n",
    "        # Create str(x, y)\n",
    "        feature_str = f\"emission:{y}+{x}\"\n",
    "        e = count_emission[(x, y)] / count_labels[y]\n",
    "        \n",
    "        # Store score\n",
    "        f[feature_str] = np.log(e)\n",
    "    \n",
    "    # Set features for emissions that are not aligned\n",
    "    for x in unique_x:\n",
    "        for y in unique_y:\n",
    "            feature_str = f\"emission:{y}+{x}\"\n",
    "            \n",
    "            # Only set to -inf if it doesn't exist as an aligned pair\n",
    "            if feature_str not in f:\n",
    "                f[feature_str] = -10**8\n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1(ii): Transition scores\n",
    "Compute $q(y_i|y_{i-1}) = \\frac{\\text{Count}(y_{i-1}, y_i)}{\\text{Count}(y_{i-1})}$ and store it into a dictionary $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_transition_scores(path):\n",
    "    '''\n",
    "    Inputs:\n",
    "        path (Path object or str): path on the local directory to the dataset to load from.\n",
    "    Outputs:\n",
    "        f (dict): Key-value mapping of str(y_{i-1}, y_i) to log(q(y_i|y_{i-1})) values.\n",
    "    '''\n",
    "    \n",
    "    count_transition = defaultdict(int) # Key is tuple (y_i, y_{i-1}), and value is Count(y_{i-1}, y_i)\n",
    "    count_labels = defaultdict(int) # Stores Count(y_i), where key is y_i\n",
    "    \n",
    "    # For computing pairs that are not aligned\n",
    "    unique_y = set([\"START\", \"STOP\"])\n",
    "\n",
    "    # Read from dataset path\n",
    "    with open(path) as f:\n",
    "        lines = f.readlines()\n",
    "        # Initialize prev_y as START\n",
    "        prev_y = \"START\"\n",
    "        \n",
    "        # Process lines\n",
    "        for line in lines:\n",
    "            # Strip newline\n",
    "            formatted_line = line.strip()\n",
    "            \n",
    "            # Only process lines that are not newlines\n",
    "            if len(formatted_line) > 0:\n",
    "                # Split into (x, y) pair\n",
    "                split_data = formatted_line.split(\" \")\n",
    "                x, y = split_data[0], split_data[1]\n",
    "                \n",
    "                transition_key = (prev_y, y)\n",
    "                count_transition[transition_key] += 1\n",
    "                count_labels[y] += 1\n",
    "                \n",
    "                # Update for next word\n",
    "                prev_y = y\n",
    "                \n",
    "                unique_y.add(y)\n",
    "            else:\n",
    "                # End of sentence\n",
    "                # Store Count(STOP|y_n)\n",
    "                transition_key = (prev_y, \"STOP\")\n",
    "                count_transition[transition_key] += 1\n",
    "                \n",
    "                # Start of next sentence: initialize prev_y to \"START\" and store Count(START)\n",
    "                prev_y = \"START\"\n",
    "                count_labels[prev_y] += 1\n",
    "    \n",
    "    # Result dictionary that maps str(x, y) to log(e(x|y)) values\n",
    "    f = {}\n",
    "    \n",
    "    # Estimate e(x|y) based on counts\n",
    "    for prev_y, y in count_transition:\n",
    "        # Create str(y_{i-1}, y_i)\n",
    "        feature_str = f\"transition:{prev_y}+{y}\"\n",
    "        q = count_transition[(prev_y, y)] / count_labels[prev_y]\n",
    "        \n",
    "        # Store score\n",
    "        f[feature_str] = np.log(q)\n",
    "    \n",
    "    # Set features for transitions that are not aligned\n",
    "    for prev_y in unique_y:\n",
    "        for y in unique_y:\n",
    "            feature_str = f\"transition:{prev_y}+{y}\"\n",
    "            \n",
    "            # Only set to -inf if it doesn't exist as an aligned pair\n",
    "            if feature_str not in f:\n",
    "                f[feature_str] = -10**8\n",
    "\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute emission and transition parameters\n",
    "f_emission_train = calculate_emission_scores(data_dir / dataset / \"train\")\n",
    "f_transition_train = calculate_transition_scores(data_dir / dataset / \"train\")\n",
    "\n",
    "# Combine the transition and emission dictionaries together\n",
    "f = {**f_emission_train, **f_transition_train}\n",
    "# Ensure the number of elements is correct\n",
    "assert(len(f) == len(f_emission_train) + len(f_transition_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2(i)\n",
    "Compute CRF scores for a given input and output sequence pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_crf_score(x, y, feature_dict):\n",
    "    ''' \n",
    "    Inputs:\n",
    "        x (list[str]): Complete input word sentence (without START or STOP tags)\n",
    "        y (list[str]): Complete output label sequence\n",
    "        feature_dict (dict[str] -> float): Dictionary that maps a given feature to its score.\n",
    "    Outputs:\n",
    "        p (float): Score given by p(y | x)\n",
    "    '''\n",
    "    \n",
    "    # Input and output sequences must be of the same length\n",
    "    assert(len(x) == len(y))\n",
    "    n = len(x) # Sequence length\n",
    "    \n",
    "    # Stores the number of times each feature appears in (x, y)\n",
    "    feature_count = defaultdict(int)\n",
    "    \n",
    "    # Compute emission features\n",
    "    for i in range(n):\n",
    "        formatted_word = x[i]\n",
    "        emission_key = f\"emission:{y[i]}+{formatted_word}\"\n",
    "        feature_count[emission_key] += 1\n",
    "    \n",
    "    # Compute transition features\n",
    "    # Add START and STOP tags to y\n",
    "    updated_y = [\"START\"] + y + [\"STOP\"]\n",
    "    for i in range(1, n+2):\n",
    "        prev_y = updated_y[i-1]\n",
    "        y_i = updated_y[i]\n",
    "        transition_key = f\"transition:{prev_y}+{y_i}\"\n",
    "        feature_count[transition_key] += 1\n",
    "    \n",
    "    # Compute score\n",
    "    score = 0\n",
    "    for feature_key, count in feature_count.items():\n",
    "        weight = feature_dict[feature_key]\n",
    "        score += weight * count\n",
    "    \n",
    "    return score\n",
    "\n",
    "# # Test function\n",
    "# x = \"Great food with an awesome atmosphere !\".split()\n",
    "# y = \"O B-positive O O O B-positive O\".split()\n",
    "# compute_crf_score(x, y, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2(ii)\n",
    "Viterbi algorithm for decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_states(path):\n",
    "    '''\n",
    "    Inputs:\n",
    "        path (Path object or str): path on the local directory to the dataset to load from.\n",
    "    Outputs:\n",
    "        states (list[str]): Unique states in the dataset.\n",
    "    '''\n",
    "    states = set()\n",
    "    \n",
    "    # Read from dataset path\n",
    "    with open(path) as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "        # Process lines\n",
    "        for line in lines:\n",
    "            # Strip newline\n",
    "            formatted_line = line.strip()\n",
    "            \n",
    "            # Only process lines that are not newlines\n",
    "            if len(formatted_line) > 0:\n",
    "                # Split into (x, y) pair\n",
    "                split_data = formatted_line.split(\" \")\n",
    "                x, y = split_data[0], split_data[1]\n",
    "                states.add(y)\n",
    "    \n",
    "    return list(states)\n",
    "\n",
    "def viterbi_decode(x, states, feature_dict, default_index=0):\n",
    "    '''\n",
    "    Inputs:\n",
    "        x (list[str]): Input sequence.\n",
    "        states (list[str]): Possible output states.\n",
    "        feature_dict (dict[str] -> float): Dictionary that maps a given feature to its score.\n",
    "        default_index (int, optional): Index to default for backpointer if scores are nan.\n",
    "    Outputs:\n",
    "        y (list[str]): Most probable output sequence.\n",
    "    '''\n",
    "    \n",
    "    n = len(x) # Number of words\n",
    "    d = len(states) # Number of states\n",
    "    scores = np.full((n, d), -np.inf) # Default state is -inf for missing values\n",
    "    bp = np.full((n, d), default_index, dtype=np.int) # Set default backpointer to the default_index state\n",
    "\n",
    "    # Convert to lowercase\n",
    "    x = [x[i] for i in range(n)]\n",
    "    \n",
    "    # Compute START transition scores\n",
    "    for i, current_y in enumerate(states):\n",
    "        transition_key = f\"transition:START+{current_y}\"\n",
    "        emission_key = f\"emission:{current_y}+{x[0]}\"\n",
    "        transmission_score = feature_dict.get(transition_key, -10**8)\n",
    "        emission_score = feature_dict.get(emission_key, -10**8)\n",
    "        scores[0, i] = transmission_score + emission_score\n",
    "    \n",
    "    # Recursively compute best scores based on transmission and emission scores at each node\n",
    "    for i in range(1, n):\n",
    "        for k, prev_y in enumerate(states):\n",
    "            for j, current_y in enumerate(states):\n",
    "                transition_key = f\"transition:{prev_y}+{current_y}\"\n",
    "                emission_key = f\"emission:{current_y}+{x[i]}\"\n",
    "                \n",
    "                transition_score = feature_dict.get(transition_key, -10**8)\n",
    "                emission_score = feature_dict.get(emission_key, -10**8)\n",
    "                overall_score = emission_score + transition_score + scores[i-1, k]\n",
    "\n",
    "                # Better score is found: Update backpointer and score arrays\n",
    "                if overall_score > scores[i, j]:\n",
    "                    scores[i, j] = overall_score\n",
    "                    bp[i,j] = k\n",
    "    \n",
    "    # Compute for STOP\n",
    "    highest_score = None\n",
    "    highest_bp = default_index\n",
    "    \n",
    "    for j, prev_y in enumerate(states):\n",
    "        transition_key = f\"transition:{prev_y}+STOP\"\n",
    "        transition_score = feature_dict.get(transition_key, -10**8)\n",
    "        overall_score = transition_score + scores[n-1, j]\n",
    "        \n",
    "        if highest_score == None or overall_score > highest_score:\n",
    "            highest_score = overall_score\n",
    "            highest_bp = j\n",
    "    \n",
    "    # Follow backpointers to get output sequence\n",
    "    result = [states[highest_bp]]\n",
    "    prev_bp = highest_bp\n",
    "    for i in range(n-1, 0, -1):\n",
    "        prev_bp = bp[i, prev_bp]\n",
    "        output = states[prev_bp]\n",
    "        # Prepend result to output list\n",
    "        result = [output] + result\n",
    "    \n",
    "    return result\n",
    "\n",
    "states = get_states(data_dir / dataset / \"train\")\n",
    "# viterbi_decode(\"Great food with an awesome atmosphere !\".split(), states, f) # Should be similar or equal to y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform decoding on dev sets\n",
    "def inference(path, states, feature_dict,output_type):\n",
    "    '''\n",
    "    Given a path, perform inference on sentences in the dataset and writes it to disk.\n",
    "    \n",
    "    Inputs:\n",
    "        path (Path object or str): path on the local directory to the dataset to perform inference on.\n",
    "        states (list[str]): Unique states that can be predicted.\n",
    "        feature_dict (dict[str] -> float): Dictionary that maps a given feature to its score.\n",
    "    Outputs:\n",
    "        None\n",
    "    '''\n",
    "    \n",
    "    default_index = states.index('O')\n",
    "    sentences = []\n",
    "\n",
    "    # Write predictions to file\n",
    "    output_filename = str(path).replace(\".in\", \".{}.out\".format(output_type))\n",
    "\n",
    "    # Read from dataset path\n",
    "    with open(path) as f:\n",
    "        lines = f.readlines()\n",
    "        sentence = []\n",
    "        \n",
    "        for line in lines:\n",
    "            formatted_line = line.strip()\n",
    "            \n",
    "            # Not the end of sentence, add it to the list\n",
    "            if len(formatted_line) > 0:\n",
    "                sentence.append(formatted_line)\n",
    "            else:\n",
    "                # End of sentence\n",
    "                sentences.append(sentence)\n",
    "                sentence = []\n",
    "    \n",
    "    # Write output file\n",
    "    with open(output_filename, \"w\") as wf:\n",
    "        for sentence in sentences:\n",
    "            # Run predictions\n",
    "            pred_sentence = viterbi_decode(sentence, states, feature_dict, default_index)\n",
    "            \n",
    "            # Write original word and predicted tags\n",
    "            for i in range(len(sentence)):\n",
    "                wf.write(sentence[i] + \" \" + pred_sentence[i] + \"\\n\")\n",
    "            \n",
    "            # End of sentence, write newline\n",
    "            wf.write(\"\\n\")\n",
    "\n",
    "inference(data_dir / dataset / \"dev.in\", states, f, output_type='p2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3(i)\n",
    "Loss calculation using forward algorithm. We first define the score calculation functions for a single sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-2.00000000e+08, -1.00000003e+08, -2.00000000e+08, ...,\n",
       "         -1.00000000e+08, -1.00000005e+08, -2.00000000e+08],\n",
       "        [-1.00000000e+08, -1.00000000e+08, -1.00000000e+08, ...,\n",
       "         -1.00000000e+08, -1.00000000e+08, -1.00000000e+08],\n",
       "        [-1.00000000e+08, -1.00000000e+08, -1.00000000e+08, ...,\n",
       "         -1.00000000e+08, -1.00000000e+08, -1.00000000e+08],\n",
       "        ...,\n",
       "        [-1.00000000e+08, -1.00000000e+08, -1.00000000e+08, ...,\n",
       "         -1.00000000e+08, -1.00000000e+08, -1.00000000e+08],\n",
       "        [-1.00000000e+08, -1.00000000e+08, -1.00000000e+08, ...,\n",
       "         -1.00000000e+08, -1.00000000e+08, -1.00000000e+08],\n",
       "        [-1.00000000e+08, -1.00000000e+08, -1.00000000e+08, ...,\n",
       "         -1.00000000e+08, -1.00000000e+08, -1.00000000e+08]]), -700)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_forward_score(x, feature_dict, states):\n",
    "    '''\n",
    "    Uses the forward algorithm to compute the score for a given sequence.\n",
    "    \n",
    "    Inputs:\n",
    "        x (list[str]): Input sequence.\n",
    "        feature_dict (dict[str] -> float): Dictionary that maps a given feature to its score.\n",
    "        states (list[str]): List of unique states.\n",
    "    Outputs:\n",
    "        forward_score (float): Forward score for this sequence.\n",
    "    '''\n",
    "    \n",
    "    n = len(x) # Number of words\n",
    "    d = len(states) # Number of states\n",
    "    forward_scores = np.zeros((n, d))\n",
    "\n",
    "    # Convert to lowercase\n",
    "    x = [x[i] for i in range(n)]\n",
    "\n",
    "    # Start forward pass\n",
    "    # Compute START transition scores\n",
    "    for i, current_y in enumerate(states):\n",
    "        transition_key = f\"transition:START+{current_y}\"\n",
    "        emission_key = f\"emission:{current_y}+{x[0]}\"\n",
    "        transition_score = feature_dict.get(transition_key, -10**8)\n",
    "        emission_score = feature_dict.get(emission_key, -10**8)\n",
    "        # Sum exponentials\n",
    "        forward_scores[0, i] = transition_score + emission_score\n",
    "    \n",
    "    # Recursively compute best scores based on transmission and emission scores at each node\n",
    "    for i in range(1, n):\n",
    "        for k, current_y in enumerate(states):\n",
    "            temp_score = 0\n",
    "            for j, prev_y in enumerate(states):\n",
    "                transition_key = f\"transition:{prev_y}+{current_y}\"\n",
    "                emission_key = f\"emission:{current_y}+{x[i]}\"\n",
    "                \n",
    "                transition_score = feature_dict.get(transition_key, -10**8)\n",
    "                emission_score = feature_dict.get(emission_key, -10**8)\n",
    "                \n",
    "                # Sum exponentials\n",
    "                temp_score += np.exp(min(emission_score + transition_score + forward_scores[i-1, j], 700))\n",
    "\n",
    "            # Add to forward scores array\n",
    "            forward_scores[i, k] = np.log(temp_score) if temp_score else -10**8\n",
    "\n",
    "    # Compute for STOP\n",
    "    forward_prob = 0\n",
    "    for j, prev_y in enumerate(states):\n",
    "        transition_key = f\"transition:{prev_y}+STOP\"\n",
    "        transition_score = feature_dict.get(transition_key, -10**8)\n",
    "        # Sum exponentials\n",
    "        overall_score = np.exp(min(transition_score + forward_scores[n-1, j], 700))\n",
    "        forward_prob += overall_score\n",
    "    # End forward pass\n",
    "    \n",
    "    alpha = np.log(forward_prob) if forward_prob else -700\n",
    "    return forward_scores, alpha\n",
    "\n",
    "\n",
    "def compute_crf_loss_sample(x, y, feature_dict, states):\n",
    "    '''\n",
    "    Inputs:\n",
    "        x (list[str]): Input sequence.\n",
    "        y (list[str]): Groundtruth output sequence.\n",
    "        feature_dict (dict[str] -> float): Dictionary that maps a given feature to its score.\n",
    "        states (list[str]): List of unique states.\n",
    "    Outputs:\n",
    "        loss (float): Loss value for a single sample.\n",
    "    '''\n",
    "    first_term = compute_crf_score(x, y, feature_dict)\n",
    "    _, forward_score = compute_forward_score(x, feature_dict, states)\n",
    "    \n",
    "    loss = -(first_term - forward_score)\n",
    "    return loss\n",
    "\n",
    "# # -45.37727\n",
    "# _, a = compute_forward_score(\"Great food with an awesome atmosphere !\".split(), f, states)\n",
    "# a\n",
    "compute_forward_score(['hola', 'buenas', 'agradezco', 'a', 'todas', 'las', 'personas', 'que', 'hayan', 'puesto', 'estos', 'comentarios', 'porque', 'esto', 'la', 'verdad', 'te', 'deja', 'saber', 'lo', 'bueno', 'y', 'lo', 'malo', 'que', 'tiene', 'el', 'restaurante', 'pero', 'yo', 'recomiendo', 'a', 'todas', 'las', 'personas', 'este', 'restaurante', 'porque', 'muy', 'pocos', 'restaurantes', 'en', 'zaragoza', 'tienen', 'pruductos', 'que', 'tiene', 'el', 'puerto', 'como', 'por', 'ejemplo', 'el', 'pescado', 'es', 'de', 'muy', 'alta', 'calidad', 'y', 'la', 'carne', 'es', 'muy', 'buena', 'que', 'es', 'de', 'denominacion', 'de', 'origen', 'y', 'hay', 'platos', 'muy', 'sabrosos', 'y', 'lo', 'bueno', 'que', 'tiene', 'el', 'restaurante', 'es', 'que', 'prepara', 'sus', 'deleciosos', 'platos', 'al', 'momento', 'nunca', 'se', 'prepara', 'la', 'comida', 'y', 'luego', 'se', 'calienta', 'y', 'con', 'un', 'tiempo', 'justo', 'para', 'el', 'cliente', 'y', 'con', 'mucha', 'limpieza', 'en', 'cocina', 'y', 'con', 'mucho', 'cuidado', 'con', 'el', 'pescado', 'y', 'demas', 'pruductos', 'y', 'con', 'excelentes', 'postres', 'caseros', 'como', 'la', 'quesa', 'pasiega', ',', 'pudin', 'de', 'frutas', ',', 'bolas', 'de', 'melon', 'maceradas', 'en', 'vino', 'de', 'pedro', 'xmenez', ',', 'tarta', 'imperial', 'de', 'almendras', 'y', 'chocolate', ',', 'etc', '.'], f, states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the methods defined early to compute the loss across the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(path):\n",
    "    '''\n",
    "    Given a path, load the data from file.\n",
    "    \n",
    "    Inputs:\n",
    "        path (Path object or str): path on the local directory to the dataset to perform inference on.\n",
    "    Outputs:\n",
    "        input_sequences (list[list[str]]): List of input sequences\n",
    "        input_labels (list[list[str]]): List of input labels\n",
    "    '''\n",
    "    input_sequences = []\n",
    "    input_labels = []\n",
    "    \n",
    "    # Read from dataset path\n",
    "    with open(path) as f:\n",
    "        lines = f.readlines()\n",
    "        sentence = []\n",
    "        labels = []\n",
    "        \n",
    "        for line in lines:\n",
    "            formatted_line = line.strip()\n",
    "            \n",
    "            # Not the end of sentence, add it to the list\n",
    "            if len(formatted_line) > 0:\n",
    "                split_data = formatted_line.split(\" \")\n",
    "                x, y = split_data[0], split_data[1]\n",
    "\n",
    "                sentence.append(x)\n",
    "                labels.append(y)\n",
    "            else:\n",
    "                # End of sentence\n",
    "                input_sequences.append(sentence)\n",
    "                input_labels.append(labels)\n",
    "                sentence = []\n",
    "                labels = []\n",
    "    \n",
    "    return input_sequences, input_labels\n",
    "\n",
    "def compute_crf_loss(input_sequences, input_labels, feature_dict, states, nabla=0, regularization= False):\n",
    "    '''\n",
    "    Given a path, perform inference on sentences in the dataset and writes it to disk.\n",
    "    \n",
    "    Inputs:\n",
    "        input_sequences (list[list[str]]): List of input sequences\n",
    "        input_labels (list[list[str]]): List of input labels\n",
    "        feature_dict (dict[str] -> float): Dictionary that maps a given feature to its score.\n",
    "        states (list[str]): List of unique states.\n",
    "    Outputs:\n",
    "        loss (float): Loss value for the dataset.\n",
    "    '''\n",
    "    loss = 0\n",
    "    for i in range(len(input_sequences)):\n",
    "        sample_loss = compute_crf_loss_sample(input_sequences[i], input_labels[i], feature_dict, states)\n",
    "        loss += sample_loss\n",
    "    if regularization:\n",
    "        reg_loss = 0\n",
    "        for feature_key in feature_dict:\n",
    "            reg_loss += feature_dict[feature_key]**2\n",
    "        reg_loss = nabla*reg_loss\n",
    "        loss += reg_loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3(ii)\n",
    "Forward backward algorithm for computing gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_backward_score(x, feature_dict, states):\n",
    "    '''\n",
    "    Uses the backward algorithm to compute the score for a given sequence.\n",
    "    \n",
    "    Inputs:\n",
    "        x (list[str]): Input sequence.\n",
    "        feature_dict (dict[str] -> float): Dictionary that maps a given feature to its score.\n",
    "        states (list[str]): List of unique states.\n",
    "    Outputs:\n",
    "        backward_score (float): Forward score for this sequence.\n",
    "    '''\n",
    "    n = len(x) # Number of words\n",
    "    d = len(states) # Number of states\n",
    "    backward_scores = np.zeros((n, d))\n",
    "    \n",
    "    forward_scores, alpha = compute_forward_score(x, feature_dict, states)\n",
    "    forward_prob = np.exp(min(alpha, 700))\n",
    "\n",
    "    # Start backward pass\n",
    "    # Compute STOP transition scores\n",
    "    for j, current_y in enumerate(states):\n",
    "        transition_key = f\"transition:{current_y}+STOP\"\n",
    "        transition_score = feature_dict.get(transition_key, -10**8)\n",
    "        # Sum exponentials\n",
    "        backward_scores[n-1, j] = transition_score\n",
    "\n",
    "    # Recursively compute best scores based on transmission and emission scores at each node\n",
    "    for i in range(n-1, 0, -1):\n",
    "        for k, current_y in enumerate(states):\n",
    "            temp_score = 0\n",
    "            for j, next_y in enumerate(states):\n",
    "                transition_key = f\"transition:{current_y}+{next_y}\"\n",
    "                emission_key = f\"emission:{next_y}+{x[i]}\"\n",
    "                \n",
    "                transition_score = feature_dict.get(transition_key, -10**8)\n",
    "                emission_score = feature_dict.get(emission_key, -10**8)\n",
    "\n",
    "                # Sum exponentials\n",
    "                temp_score += np.exp(min(emission_score + transition_score + backward_scores[i, j], 700))\n",
    "\n",
    "            # Add to backward scores array\n",
    "            backward_scores[i-1, k] = np.log(temp_score) if temp_score else -10**8\n",
    "    \n",
    "    # Compute for START\n",
    "    backward_prob = 0\n",
    "    for j, next_y in enumerate(states):\n",
    "        transition_key = f\"transition:START+{next_y}\"\n",
    "        emission_key = f\"emission:{next_y}+{x[0]}\" # Emission of last word\n",
    "\n",
    "        transition_score = feature_dict.get(transition_key, -10**8)\n",
    "        emission_score = feature_dict.get(emission_key, -10**8)\n",
    "        # Sum exponentials\n",
    "        overall_score = np.exp(min(emission_score + transition_score + backward_scores[0, j], 700))\n",
    "        backward_prob += overall_score\n",
    "    # End backward pass\n",
    "    \n",
    "    beta = np.log(backward_prob) if backward_prob else -700\n",
    "    return backward_scores, beta\n",
    "\n",
    "def forward_backward(x, feature_dict, states):\n",
    "    '''\n",
    "    Uses the forward-backward algorithm to compute the expected counts for a given sequence.\n",
    "    \n",
    "    Inputs:\n",
    "        x (list[str]): Input sequence.\n",
    "        feature_dict (dict[str] -> float): Dictionary that maps a given feature to its score.\n",
    "        states (list[str]): List of unique states.\n",
    "    Outputs:\n",
    "        expected_counts (dict): Expected counts for each feature.\n",
    "    '''\n",
    "    \n",
    "    n = len(x) # Number of words\n",
    "    d = len(states) # Number of states\n",
    "    \n",
    "    forward_scores, alpha = compute_forward_score(x, feature_dict, states)\n",
    "    forward_prob = np.exp(min(alpha, 700))\n",
    "    backward_scores, beta = compute_backward_score(x, feature_dict, states)\n",
    "    backward_prob = np.exp(min(beta, 700))\n",
    "    \n",
    "    # Computed expected counts for all features\n",
    "    feature_expected_counts = defaultdict(float)\n",
    "    \n",
    "    # Compute expected emission counts\n",
    "    for i in range(n):\n",
    "        for j, current_y in enumerate(states):\n",
    "            emission_key = f\"emission:{current_y}+{x[i]}\"\n",
    "            feature_expected_counts[emission_key] += np.exp(min(forward_scores[i, j] + backward_scores[i, j] - alpha, 700))\n",
    "    \n",
    "    # Compute expected START / STOP transition counts\n",
    "    for j, next_y in enumerate(states):\n",
    "        start_transition_key = f\"transition:START+{next_y}\"\n",
    "        feature_expected_counts[start_transition_key] += np.exp(min(forward_scores[0, j] + backward_scores[0, j] - alpha, 700))\n",
    "        \n",
    "        stop_transition_key = f\"transition:{next_y}+STOP\"\n",
    "        feature_expected_counts[stop_transition_key] += np.exp(min(forward_scores[n-1, j] + backward_scores[n-1, j] - alpha, 700))\n",
    "    \n",
    "    # Compute expected transition probabilities for everything else\n",
    "    for j, current_y in enumerate(states):\n",
    "        for k, next_y in enumerate(states):\n",
    "            transition_key = f\"transition:{current_y}+{next_y}\"\n",
    "            transition_score = feature_dict.get(transition_key, -10**8)\n",
    "            \n",
    "            total = 0\n",
    "            for i in range(n-1):\n",
    "                emission_key = f\"emission:{next_y}+{x[i+1]}\"\n",
    "                emission_score = feature_dict.get(emission_key, -10**8)\n",
    "\n",
    "                total += np.exp(min(forward_scores[i, j] + backward_scores[i+1, k] + transition_score + emission_score - alpha, 700))\n",
    "\n",
    "            feature_expected_counts[transition_key] = total\n",
    "    \n",
    "    return feature_expected_counts\n",
    "\n",
    "# # 2.24089\n",
    "# forward_backward(\"Great food with an awesome atmosphere !\".split(), f, states)[\"transition:O+O\"]\n",
    "forward_backward(['hola', 'buenas', 'agradezco', 'a', 'todas', 'las', 'personas', 'que', 'hayan', 'puesto', 'estos', 'comentarios', 'porque', 'esto', 'la', 'verdad', 'te', 'deja', 'saber', 'lo', 'bueno', 'y', 'lo', 'malo', 'que', 'tiene', 'el', 'restaurante', 'pero', 'yo', 'recomiendo', 'a', 'todas', 'las', 'personas', 'este', 'restaurante', 'porque', 'muy', 'pocos', 'restaurantes', 'en', 'zaragoza', 'tienen', 'pruductos', 'que', 'tiene', 'el', 'puerto', 'como', 'por', 'ejemplo', 'el', 'pescado', 'es', 'de', 'muy', 'alta', 'calidad', 'y', 'la', 'carne', 'es', 'muy', 'buena', 'que', 'es', 'de', 'denominacion', 'de', 'origen', 'y', 'hay', 'platos', 'muy', 'sabrosos', 'y', 'lo', 'bueno', 'que', 'tiene', 'el', 'restaurante', 'es', 'que', 'prepara', 'sus', 'deleciosos', 'platos', 'al', 'momento', 'nunca', 'se', 'prepara', 'la', 'comida', 'y', 'luego', 'se', 'calienta', 'y', 'con', 'un', 'tiempo', 'justo', 'para', 'el', 'cliente', 'y', 'con', 'mucha', 'limpieza', 'en', 'cocina', 'y', 'con', 'mucho', 'cuidado', 'con', 'el', 'pescado', 'y', 'demas', 'pruductos', 'y', 'con', 'excelentes', 'postres', 'caseros', 'como', 'la', 'quesa', 'pasiega', ',', 'pudin', 'de', 'frutas', ',', 'bolas', 'de', 'melon', 'maceradas', 'en', 'vino', 'de', 'pedro', 'xmenez', ',', 'tarta', 'imperial', 'de', 'almendras', 'y', 'chocolate', ',', 'etc', '.'], f, states)[\"transition:O+O\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_count(x, y, feature_dict):\n",
    "    ''' \n",
    "    Inputs:\n",
    "        x (list[str]): Complete input word sentence (without START or STOP tags)\n",
    "        y (list[str]): Complete output label sequence\n",
    "        feature_dict (dict[str] -> float): Dictionary that maps a given feature to its score.\n",
    "    Outputs:\n",
    "        p (float): Score given by p(y | x)\n",
    "    '''\n",
    "    \n",
    "    # Input and output sequences must be of the same length\n",
    "    assert(len(x) == len(y))\n",
    "    n = len(x) # Sequence length\n",
    "    \n",
    "    # Stores the number of times each feature appears in (x, y)\n",
    "    feature_count = defaultdict(int)\n",
    "    \n",
    "    # Compute emission features\n",
    "    for i in range(n):\n",
    "        formatted_word = x[i]\n",
    "        emission_key = f\"emission:{y[i]}+{formatted_word}\"\n",
    "        feature_count[emission_key] += 1\n",
    "    \n",
    "    # Compute transition features\n",
    "    # Add START and STOP tags to y\n",
    "    updated_y = [\"START\"] + y + [\"STOP\"]\n",
    "    for i in range(1, n+2):\n",
    "        prev_y = updated_y[i-1]\n",
    "        y_i = updated_y[i]\n",
    "        transition_key = f\"transition:{prev_y}+{y_i}\"\n",
    "        feature_count[transition_key] += 1\n",
    "    \n",
    "    return feature_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wish to compute the gradient:\n",
    "$\\frac{\\delta L(w)}{\\delta \\lambda_k} = \\sum_i E_{p(y|x_i)} [ f_k (x_i, y) ] - \\sum_i f_k (x_i, y_i) $\n",
    "\n",
    "which can be computed by subtracting the actual counts of the features from the expected counts of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_gradients(train_inputs, train_labels, f, states, nabla = 0, regularization = False):\n",
    "    '''\n",
    "    Uses the forward-backward algorithm to compute gradients analytically.\n",
    "    \n",
    "    Inputs:\n",
    "        train_inputs (list[str]): Input sequence.\n",
    "        train_labels (list[str]): Input labels.\n",
    "        f (dict[str] -> float): Dictionary that maps a given feature to its score.\n",
    "        states (list[str]): List of unique states.\n",
    "    Outputs:\n",
    "        backward_score (float): Backward score for this sequence.\n",
    "        feature_gradients (dict[str] -> float): Dictionary that maps a given feature to its analytical gradient.\n",
    "    '''\n",
    "    \n",
    "    feature_gradients = defaultdict(float)\n",
    "\n",
    "    for i in range(len(train_labels)):\n",
    "        x = train_inputs[i]\n",
    "        y = train_labels[i]\n",
    "\n",
    "        feature_expected_counts = forward_backward(x, f, states)\n",
    "        actual_counts = get_feature_count(x, y, f)\n",
    "\n",
    "        for k, v in feature_expected_counts.items():\n",
    "            feature_gradients[k] += v\n",
    "\n",
    "        for k, v in actual_counts.items():\n",
    "            feature_gradients[k] -= v\n",
    "    if regularization:\n",
    "        for k, v in feature_expected_counts.items():\n",
    "            feature_gradients[k] += 2*nabla*f[k]\n",
    "\n",
    "    return feature_gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the results numerically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = get_states(data_dir / dataset / \"train\")\n",
    "train_inputs, train_labels = get_dataset(data_dir / dataset / \"train\")\n",
    "\n",
    "# Comment the below section out for Spanish, as O+the doesn't exist!\n",
    "# Check against numerical gradient for several values is equal\n",
    "feature_key_checks = ['emission:O+the', 'transition:START+O', 'transition:O+O', 'transition:I-negative+I-positive']\n",
    "feature_gradients = compute_gradients(train_inputs, train_labels, f, states)\n",
    "loss1 = compute_crf_loss(train_inputs, train_labels, f, states)\n",
    "delta = 1e-6\n",
    "\n",
    "for feature_key in feature_key_checks:\n",
    "    print(\"Checking\", feature_key)\n",
    "    new_f = f.copy()\n",
    "    new_f[feature_key] += delta\n",
    "\n",
    "    loss2 = compute_crf_loss(train_inputs, train_labels, new_f, states)\n",
    "\n",
    "    numerical_gradient = (loss2 - loss1) / delta\n",
    "    analytical_gradient = feature_gradients[feature_key]\n",
    "    \n",
    "    # Ensure numerical and analytical gradient are close\n",
    "    assert(abs(numerical_gradient - analytical_gradient) / max(abs(numerical_gradient), 1e-8) <= 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are helper functions for dictionary to array conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_to_dict(w, f, reverse = False):\n",
    "    '''\n",
    "    Converts a numpy array w to a dictionary with keys from f.\n",
    "    '''\n",
    "    for i,k in enumerate(f.keys()):\n",
    "        f[k] = w[i]\n",
    "    return f\n",
    "def prepare_grad_for_bfgs(grads,f):\n",
    "    '''\n",
    "    Converts a dictionary to a numpy array.\n",
    "    '''\n",
    "    np_grads = np.zeros(len(f))\n",
    "    for i,k in enumerate(f.keys()):\n",
    "        np_grads[i] = grads[k]\n",
    "    return np_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running L-BFGS on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:19007.6578\n",
      "Loss:12068.6881\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-7a92cc05ecd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0minit_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfmin_l_bfgs_b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_loss_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpgtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbackF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36mfmin_l_bfgs_b\u001b[0;34m(func, x0, fprime, args, approx_grad, bounds, m, factr, pgtol, epsilon, iprint, maxfun, maxiter, disp, callback, maxls)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     res = _minimize_lbfgsb(fun, x0, args=args, jac=jac, bounds=bounds,\n\u001b[0;32m--> 199\u001b[0;31m                            **opts)\n\u001b[0m\u001b[1;32m    200\u001b[0m     d = {'grad': res['jac'],\n\u001b[1;32m    201\u001b[0m          \u001b[0;34m'task'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, **unknown_options)\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;31m# Overwrite f and g:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'NEW_X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;31m# new iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36mfunc_and_grad\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m             \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[0;34m(*wrapper_args)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-7a92cc05ecd2>\u001b[0m in \u001b[0;36mget_loss_grad\u001b[0;34m(w, *args)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# compute loss and grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_crf_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_grad_for_bfgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# return loss and grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-5750ccd33880>\u001b[0m in \u001b[0;36mcompute_gradients\u001b[0;34m(train_inputs, train_labels, f, states, nabla, regularization)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mfeature_expected_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mactual_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_feature_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-aad77fc581c0>\u001b[0m in \u001b[0;36mforward_backward\u001b[0;34m(x, feature_dict, states)\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0memission_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memission_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m                 \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforward_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbackward_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtransition_score\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0memission_score\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m700\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mfeature_expected_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransition_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from scipy.optimize import fmin_l_bfgs_b \n",
    "\n",
    "states = get_states(data_dir / dataset / \"train\")\n",
    "train_inputs, train_labels = get_dataset(data_dir / dataset / \"train\")\n",
    "\n",
    "def callbackF(w):\n",
    "    '''\n",
    "    This function will be called by \"fmin_l_bfgs_b\"\n",
    "    Arg:\n",
    "        w: weights, numpy array\n",
    "    '''\n",
    "    loss = compute_crf_loss(train_inputs,train_labels,f,states,0.1,regularization=True) \n",
    "    print('Loss:{0:.4f}'.format(loss))\n",
    "\n",
    "def get_loss_grad(w,*args): \n",
    "    '''\n",
    "    This function will be called by \"fmin_l_bfgs_b\"\n",
    "    Arg:\n",
    "        w: weights, numpy array\n",
    "    Returns:\n",
    "        loss: loss, float\n",
    "        grads: gradients, numpy array\n",
    "    '''\n",
    "\n",
    "    train_inputs,train_labels,f,states = args\n",
    "    f = numpy_to_dict(w,f)\n",
    "    # compute loss and grad\n",
    "    loss = compute_crf_loss(train_inputs, train_labels, f, states, 0.1, regularization=True)\n",
    "    grads = compute_gradients(train_inputs, train_labels, f, states, 0.1, regularization=True)\n",
    "    grads = prepare_grad_for_bfgs(grads, f)\n",
    "    # return loss and grad\n",
    "    return loss, grads\n",
    "\n",
    "init_w = np.zeros(len(f))\n",
    "result = fmin_l_bfgs_b(get_loss_grad, init_w, args=(train_inputs, train_labels, f, states), pgtol=0.01, callback=callbackF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = numpy_to_dict(result[0], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference(data_dir / dataset / \"dev.in\", states, f, \"p4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
