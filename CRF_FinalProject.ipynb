{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "data_dir = Path(\"data/\")\n",
    "dataset = \"EN\" # EN or ES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1(i): Emission scores\n",
    "Compute $e(x|y) = \\frac{\\text{Count}(y \\rightarrow x)}{\\text{Count}(y)}$ and store it into a dictionary $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_emission_scores(path):\n",
    "    '''\n",
    "    Inputs:\n",
    "        path (Path object or str): path on the local directory to the dataset to load from.\n",
    "    Outputs:\n",
    "        f (dict): Key-value mapping of str(x, y) to log(e(x|y)) values.\n",
    "    '''\n",
    "\n",
    "    count_emission = defaultdict(int) # Stores Count(y -> x), where key is tuple (x, y), and value is Count(y -> x)\n",
    "    count_labels = defaultdict(int) # Stores Count(y), where key is y\n",
    "    \n",
    "    # For computing pairs that are not aligned\n",
    "    unique_x = set()\n",
    "    unique_y = set()\n",
    "    \n",
    "    # Read from dataset path\n",
    "    with open(path) as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "        # Process lines\n",
    "        for line in lines:\n",
    "            # Strip newline\n",
    "            formatted_line = line.strip()\n",
    "            \n",
    "            # Only process lines that are not newlines\n",
    "            if len(formatted_line) > 0:\n",
    "                # Split into (x, y) pair\n",
    "                split_data = formatted_line.split(\" \")\n",
    "                x, y = split_data[0].lower(), split_data[1]\n",
    "\n",
    "                count_emission[(x, y)] += 1\n",
    "                count_labels[y] += 1\n",
    "                \n",
    "                unique_x.add(x)\n",
    "                unique_y.add(y)\n",
    "    \n",
    "    # Result dictionary that maps str(x, y) to log(e(x|y)) values\n",
    "    f = {}\n",
    "    \n",
    "    # Estimate e(x|y) based on counts\n",
    "    for x, y in count_emission:\n",
    "        # Create str(x, y)\n",
    "        feature_str = f\"emission:{y}+{x}\"\n",
    "        e = count_emission[(x, y)] / count_labels[y]\n",
    "        \n",
    "        # Store score\n",
    "        f[feature_str] = np.log(e)\n",
    "    \n",
    "    # Set features for emissions that are not aligned\n",
    "    for x in unique_x:\n",
    "        for y in unique_y:\n",
    "            feature_str = f\"emission:{y}+{x}\"\n",
    "            \n",
    "            # Only set to -inf if it doesn't exist as an aligned pair\n",
    "            if feature_str not in f:\n",
    "                f[feature_str] = -np.inf\n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1(ii): Transition scores\n",
    "Compute $q(y_i|y_{i-1}) = \\frac{\\text{Count}(y_{i-1}, y_i)}{\\text{Count}(y_{i-1})}$ and store it into a dictionary $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_transition_scores(path):\n",
    "    '''\n",
    "    Inputs:\n",
    "        path (Path object or str): path on the local directory to the dataset to load from.\n",
    "    Outputs:\n",
    "        f (dict): Key-value mapping of str(y_{i-1}, y_i) to log(q(y_i|y_{i-1})) values.\n",
    "    '''\n",
    "    \n",
    "    count_transition = defaultdict(int) # Key is tuple (y_i, y_{i-1}), and value is Count(y_{i-1}, y_i)\n",
    "    count_labels = defaultdict(int) # Stores Count(y_i), where key is y_i\n",
    "    \n",
    "    # For computing pairs that are not aligned\n",
    "    unique_y = set()\n",
    "\n",
    "    # Read from dataset path\n",
    "    with open(path) as f:\n",
    "        lines = f.readlines()\n",
    "        # Initialize prev_y as START\n",
    "        prev_y = \"START\"\n",
    "        \n",
    "        # Process lines\n",
    "        for line in lines:\n",
    "            # Strip newline\n",
    "            formatted_line = line.strip()\n",
    "            \n",
    "            # Only process lines that are not newlines\n",
    "            if len(formatted_line) > 0:\n",
    "                # Split into (x, y) pair\n",
    "                split_data = formatted_line.split(\" \")\n",
    "                x, y = split_data[0].lower(), split_data[1]\n",
    "                \n",
    "                transition_key = (prev_y, y)\n",
    "                count_transition[transition_key] += 1\n",
    "                count_labels[y] += 1\n",
    "                \n",
    "                # Update for next word\n",
    "                prev_y = y\n",
    "                \n",
    "                unique_y.add(y)\n",
    "            else:\n",
    "                # End of sentence\n",
    "                # Store Count(STOP|y_n)\n",
    "                transition_key = (prev_y, \"STOP\")\n",
    "                count_transition[transition_key] += 1\n",
    "                \n",
    "                # Start of next sentence: initialize prev_y to \"START\" and store Count(START)\n",
    "                prev_y = \"START\"\n",
    "                count_labels[prev_y] += 1\n",
    "    \n",
    "    # Result dictionary that maps str(x, y) to log(e(x|y)) values\n",
    "    f = {}\n",
    "    \n",
    "    # Estimate e(x|y) based on counts\n",
    "    for prev_y, y in count_transition:\n",
    "        # Create str(y_{i-1}, y_i)\n",
    "        feature_str = f\"transition:{prev_y}+{y}\"\n",
    "        q = count_transition[(prev_y, y)] / count_labels[prev_y]\n",
    "        \n",
    "        # Store score\n",
    "        f[feature_str] = np.log(q)\n",
    "    \n",
    "    # Set features for transitions that are not aligned\n",
    "    for prev_y in unique_y:\n",
    "        for y in unique_y:\n",
    "            feature_str = f\"transition:{prev_y}+{y}\"\n",
    "            \n",
    "            # Only set to -inf if it doesn't exist as an aligned pair\n",
    "            if feature_str not in f:\n",
    "                f[feature_str] = -np.inf\n",
    "\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute emission and transition parameters\n",
    "f_emission_train = calculate_emission_scores(data_dir / dataset / \"train\")\n",
    "f_transition_train = calculate_transition_scores(data_dir / dataset / \"train\")\n",
    "\n",
    "# Combine the transition and emission dictionaries together\n",
    "f = {**f_emission_train, **f_transition_train}\n",
    "# Ensure the number of elements is correct\n",
    "assert(len(f) == len(f_emission_train) + len(f_transition_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2(i)\n",
    "Compute CRF scores for a given input and output sequence pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-44.57667948595218"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_crf_score(x, y, feature_dict):\n",
    "    ''' \n",
    "    Inputs:\n",
    "        x (list[str]): Complete input word sentence (without START or STOP tags)\n",
    "        y (list[str]): Complete output label sequence\n",
    "        feature_dict (dict[str] -> float): Dictionary that maps a given feature to its score.\n",
    "    Outputs:\n",
    "        p (float): Score given by p(y | x)\n",
    "    '''\n",
    "    \n",
    "    # Input and output sequences must be of the same length\n",
    "    assert(len(x) == len(y))\n",
    "    n = len(x) # Sequence length\n",
    "    \n",
    "    # Stores the number of times each feature appears in (x, y)\n",
    "    feature_count = defaultdict(int)\n",
    "    \n",
    "    # Compute emission features\n",
    "    for i in range(n):\n",
    "        formatted_word = x[i].lower()\n",
    "        emission_key = f\"emission:{y[i]}+{formatted_word}\"\n",
    "        feature_count[emission_key] += 1\n",
    "    \n",
    "    # Compute transition features\n",
    "    # Add START and STOP tags to y\n",
    "    updated_y = [\"START\"] + y + [\"STOP\"]\n",
    "    for i in range(1, n+2):\n",
    "        prev_y = updated_y[i-1]\n",
    "        y_i = updated_y[i]\n",
    "        transition_key = f\"transition:{prev_y}+{y_i}\"\n",
    "        feature_count[transition_key] += 1\n",
    "    \n",
    "    # Compute score\n",
    "    score = 0\n",
    "    for feature_key, count in feature_count.items():\n",
    "        weight = feature_dict[feature_key]\n",
    "        score += weight * count\n",
    "    \n",
    "    return score\n",
    "\n",
    "# Test function\n",
    "x = \"Great food with an awesome atmosphere !\".split()\n",
    "y = \"O B-positive O O O B-positive O\".split()\n",
    "compute_crf_score(x, y, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2(ii)\n",
    "Viterbi algorithm for decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I-negative',\n",
       " 'I-negative',\n",
       " 'I-negative',\n",
       " 'I-negative',\n",
       " 'I-negative',\n",
       " 'I-negative',\n",
       " 'B-neutral']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_states(path):\n",
    "    '''\n",
    "    Inputs:\n",
    "        path (Path object or str): path on the local directory to the dataset to load from.\n",
    "    Outputs:\n",
    "        states (list[str]): Unique states in the dataset.\n",
    "    '''\n",
    "    states = set()\n",
    "    \n",
    "    # Read from dataset path\n",
    "    with open(path) as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "        # Process lines\n",
    "        for line in lines:\n",
    "            # Strip newline\n",
    "            formatted_line = line.strip()\n",
    "            \n",
    "            # Only process lines that are not newlines\n",
    "            if len(formatted_line) > 0:\n",
    "                # Split into (x, y) pair\n",
    "                split_data = formatted_line.split(\" \")\n",
    "                x, y = split_data[0], split_data[1]\n",
    "                states.add(y)\n",
    "    \n",
    "    return list(states)\n",
    "\n",
    "def viterbi_decode(x, states, feature_dict, default_index=0):\n",
    "    '''\n",
    "    Inputs:\n",
    "        x (list[str]): Input sequence.\n",
    "        states (list[str]): Possible output states.\n",
    "        feature_dict (dict[str] -> float): Dictionary that maps a given feature to its score.\n",
    "        default_index (int, optional): Index to default for backpointer if scores are nan.\n",
    "    Outputs:\n",
    "        y (list[str]): Most probable output sequence.\n",
    "    '''\n",
    "    \n",
    "    n = len(x) # Number of words\n",
    "    d = len(states) # Number of states\n",
    "    scores = np.full((n, d), -np.inf) # Default state is -inf for missing values\n",
    "    bp = np.full((n, d), default_index, dtype=np.int) # Set default backpointer to the default_index state\n",
    "\n",
    "    # Convert to lowercase\n",
    "    x = [x[i].lower() for i in range(n)]\n",
    "    \n",
    "    # Compute START transition scores\n",
    "    for i, current_y in enumerate(states):\n",
    "        transition_key = f\"transition:START+{current_y}\"\n",
    "        emission_key = f\"emission:{current_y}+{x[0]}\"\n",
    "        if transition_key in feature_dict and emission_key in feature_dict:\n",
    "            transmission_score = feature_dict[transition_key]\n",
    "            emission_score = feature_dict[emission_key]\n",
    "            scores[0, i] = transmission_score + emission_score\n",
    "    \n",
    "    # Recursively compute best scores based on transmission and emission scores at each node\n",
    "    for i in range(1, n):\n",
    "        for k, prev_y in enumerate(states):\n",
    "            for j, current_y in enumerate(states):\n",
    "                transition_key = f\"transition:{prev_y}+{current_y}\"\n",
    "                emission_key = f\"emission:{current_y}+{x[i]}\"\n",
    "                \n",
    "                # Only consider if the feature exists\n",
    "                if transition_key in feature_dict and emission_key in feature_dict:\n",
    "                    transition_score = feature_dict[transition_key]\n",
    "                    emission_score = feature_dict[emission_key]\n",
    "                    overall_score = emission_score + transition_score + scores[i-1, k]\n",
    "                    \n",
    "                    # Better score is found: Update backpointer and score arrays\n",
    "                    if overall_score > scores[i, j]:\n",
    "                        scores[i, j] = overall_score\n",
    "                        bp[i,j] = k\n",
    "    \n",
    "    # Compute for STOP\n",
    "    highest_score = None\n",
    "    highest_bp = default_index\n",
    "    \n",
    "    for j, prev_y in enumerate(states):\n",
    "        if not np.isnan(scores[n-1, j]):\n",
    "            transition_key = f\"transition:{prev_y}+STOP\"\n",
    "\n",
    "            if transition_key in feature_dict:\n",
    "                transition_score = feature_dict[transition_key]\n",
    "                overall_score = transition_score + scores[n-1, j]\n",
    "                if highest_score == None or overall_score > highest_score:\n",
    "                    highest_score = overall_score\n",
    "                    highest_bp = j\n",
    "    \n",
    "    # Follow backpointers to get output sequence\n",
    "    result = [states[highest_bp]]\n",
    "    prev_bp = highest_bp\n",
    "    for i in range(n-1, 0, -1):\n",
    "        prev_bp = bp[i, prev_bp]\n",
    "        output = states[prev_bp]\n",
    "        # Prepend result to output list\n",
    "        result = [output] + result\n",
    "    \n",
    "    return result\n",
    "\n",
    "states = get_states(data_dir / dataset / \"train\")\n",
    "viterbi_decode(\"Great food with an hip atmosphere !\".split(), states, f) # Should be similar or equal to y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform decoding on dev sets\n",
    "def inference(path, states, feature_dict):\n",
    "    '''\n",
    "    Given a path, perform inference on sentences in the dataset and writes it to disk.\n",
    "    \n",
    "    Inputs:\n",
    "        path (Path object or str): path on the local directory to the dataset to perform inference on.\n",
    "        states (list[str]): Unique states that can be predicted.\n",
    "        feature_dict (dict[str] -> float): Dictionary that maps a given feature to its score.\n",
    "    Outputs:\n",
    "        None\n",
    "    '''\n",
    "    \n",
    "    default_index = states.index('O')\n",
    "    sentences = []\n",
    "\n",
    "    # Write predictions to file\n",
    "    output_filename = str(path).replace(\".in\", \".p2.out\")\n",
    "\n",
    "    # Read from dataset path\n",
    "    with open(path) as f:\n",
    "        lines = f.readlines()\n",
    "        sentence = []\n",
    "        \n",
    "        for line in lines:\n",
    "            formatted_line = line.strip()\n",
    "            \n",
    "            # Not the end of sentence, add it to the list\n",
    "            if len(formatted_line) > 0:\n",
    "                sentence.append(formatted_line)\n",
    "            else:\n",
    "                # End of sentence\n",
    "                sentences.append(sentence)\n",
    "                sentence = []\n",
    "    \n",
    "    # Write output file\n",
    "    with open(output_filename, \"w\") as wf:\n",
    "        for sentence in sentences:\n",
    "            # Run predictions\n",
    "            pred_sentence = viterbi_decode(sentence, states, feature_dict, default_index)\n",
    "            \n",
    "            # Write original word and predicted tags\n",
    "            for i in range(len(sentence)):\n",
    "                wf.write(sentence[i] + \" \" + pred_sentence[i] + \"\\n\")\n",
    "            \n",
    "            # End of sentence, write newline\n",
    "            wf.write(\"\\n\")\n",
    "\n",
    "inference(data_dir / dataset / \"dev.in\", states, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3(i)\n",
    "Loss calculation using forward algorithm. We first define the score calculation functions for a single sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_forward_score(x, feature_dict, states):\n",
    "    '''\n",
    "    Uses the forward algorithm to compute the score for a given sequence.\n",
    "    \n",
    "    Inputs:\n",
    "        x (list[str]): Input sequence.\n",
    "        feature_dict (dict[str] -> float): Dictionary that maps a given feature to its score.\n",
    "        states (list[str]): List of unique states.\n",
    "    Outputs:\n",
    "        forward_score (float): Forward score for this sequence.\n",
    "    '''\n",
    "    \n",
    "    n = len(x) # Number of words\n",
    "    d = len(states) # Number of states\n",
    "    forward_scores = np.zeros((n, d))\n",
    "    backward_scores = np.zeros((n, d))\n",
    "\n",
    "    # Convert to lowercase\n",
    "    x = [x[i].lower() for i in range(n)]\n",
    "\n",
    "    # Start forward pass\n",
    "    # Compute START transition scores\n",
    "    for i, current_y in enumerate(states):\n",
    "        transition_key = f\"transition:START+{current_y}\"\n",
    "        emission_key = f\"emission:{current_y}+{x[0]}\"\n",
    "        if transition_key in feature_dict and emission_key in feature_dict:\n",
    "            transition_score = feature_dict[transition_key]\n",
    "            emission_score = feature_dict[emission_key]\n",
    "            # Sum exponentials\n",
    "            forward_scores[0, i] = np.exp(transition_score + emission_score)\n",
    "    \n",
    "    # Recursively compute best scores based on transmission and emission scores at each node\n",
    "    for i in range(1, n):\n",
    "        for k, prev_y in enumerate(states):\n",
    "            for j, current_y in enumerate(states):\n",
    "                transition_key = f\"transition:{prev_y}+{current_y}\"\n",
    "                emission_key = f\"emission:{current_y}+{x[i]}\"\n",
    "                \n",
    "                # Only consider if the feature exists\n",
    "                if transition_key in feature_dict and emission_key in feature_dict:\n",
    "                    transition_score = feature_dict[transition_key]\n",
    "                    emission_score = feature_dict[emission_key]\n",
    "                    # Sum exponentials\n",
    "                    overall_score = np.exp(emission_score + transition_score) * forward_scores[i-1, k]\n",
    "\n",
    "                    # Add to forward scores array\n",
    "                    forward_scores[i, j] += overall_score\n",
    "\n",
    "    # Compute for STOP\n",
    "    forward_prob = 0\n",
    "    for j, prev_y in enumerate(states):\n",
    "        transition_key = f\"transition:{prev_y}+STOP\"\n",
    "        \n",
    "        if transition_key in feature_dict:\n",
    "            transition_score = feature_dict[transition_key]\n",
    "            # Sum exponentials\n",
    "            overall_score = np.exp(transition_score) * forward_scores[n-1, j]\n",
    "            forward_prob += overall_score\n",
    "    # End forward pass\n",
    "    \n",
    "    forward_score = np.log(forward_prob)\n",
    "    return forward_score\n",
    "\n",
    "\n",
    "def compute_crf_loss_sample(x, y, feature_dict, states):\n",
    "    '''\n",
    "    Inputs:\n",
    "        x (list[str]): Input sequence.\n",
    "        y (list[str]): Groundtruth output sequence.\n",
    "        feature_dict (dict[str] -> float): Dictionary that maps a given feature to its score.\n",
    "        states (list[str]): List of unique states.\n",
    "    Outputs:\n",
    "        loss (float): Loss value for a single sample.\n",
    "    '''\n",
    "    first_term = compute_crf_score(x, y, feature_dict)\n",
    "    forward_score = compute_forward_score(x, feature_dict, states)\n",
    "    \n",
    "    loss = -(first_term - forward_score)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the methods defined early to compute the loss across the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(path):\n",
    "    '''\n",
    "    Given a path, load the data from file.\n",
    "    \n",
    "    Inputs:\n",
    "        path (Path object or str): path on the local directory to the dataset to perform inference on.\n",
    "    Outputs:\n",
    "        input_sequences (list[list[str]]): List of input sequences\n",
    "        input_labels (list[list[str]]): List of input labels\n",
    "    '''\n",
    "    input_sequences = []\n",
    "    input_labels = []\n",
    "    \n",
    "    # Read from dataset path\n",
    "    with open(path) as f:\n",
    "        lines = f.readlines()\n",
    "        sentence = []\n",
    "        labels = []\n",
    "        \n",
    "        for line in lines:\n",
    "            formatted_line = line.strip()\n",
    "            \n",
    "            # Not the end of sentence, add it to the list\n",
    "            if len(formatted_line) > 0:\n",
    "                split_data = formatted_line.split(\" \")\n",
    "                x, y = split_data[0].lower(), split_data[1]\n",
    "\n",
    "                sentence.append(x)\n",
    "                labels.append(y)\n",
    "            else:\n",
    "                # End of sentence\n",
    "                input_sequences.append(sentence)\n",
    "                input_labels.append(labels)\n",
    "                sentence = []\n",
    "                labels = []\n",
    "    \n",
    "    return input_sequences, input_labels\n",
    "\n",
    "def compute_crf_loss(input_sequences, input_labels, feature_dict, states):\n",
    "    '''\n",
    "    Given a path, perform inference on sentences in the dataset and writes it to disk.\n",
    "    \n",
    "    Inputs:\n",
    "        input_sequences (list[list[str]]): List of input sequences\n",
    "        input_labels (list[list[str]]): List of input labels\n",
    "        feature_dict (dict[str] -> float): Dictionary that maps a given feature to its score.\n",
    "        states (list[str]): List of unique states.\n",
    "    Outputs:\n",
    "        loss (float): Loss value for the dataset.\n",
    "    '''\n",
    "    loss = 0\n",
    "    for i in range(len(input_sequences)):\n",
    "        sample_loss = compute_crf_loss_sample(input_sequences[i], input_labels[i], feature_dict, states)\n",
    "        loss += sample_loss\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3(ii)\n",
    "Forward backward algorithm for computing gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_backward(x, feature_dict, states):\n",
    "    '''\n",
    "    Uses the backward algorithm to compute the score for a given sequence.\n",
    "    \n",
    "    Inputs:\n",
    "        x (list[str]): Input sequence.\n",
    "        feature_dict (dict[str] -> float): Dictionary that maps a given feature to its score.\n",
    "        states (list[str]): List of unique states.\n",
    "    Outputs:\n",
    "        backward_score (float): Backward score for this sequence.\n",
    "    '''\n",
    "    \n",
    "    n = len(x) # Number of words\n",
    "    d = len(states) # Number of states\n",
    "    forward_scores = np.zeros((n, d))\n",
    "    backward_scores = np.zeros((n, d))\n",
    "\n",
    "    # Convert to lowercase\n",
    "    x = [x[i].lower() for i in range(n)]\n",
    "\n",
    "    # Start forward pass\n",
    "    # Compute START transition scores\n",
    "    for i, current_y in enumerate(states):\n",
    "        transition_key = f\"transition:START+{current_y}\"\n",
    "        emission_key = f\"emission:{current_y}+{x[0]}\"\n",
    "        if transition_key in feature_dict and emission_key in feature_dict:\n",
    "            transition_score = feature_dict[transition_key]\n",
    "            emission_score = feature_dict[emission_key]\n",
    "            # Sum exponentials\n",
    "            forward_scores[0, i] = np.exp(transition_score + emission_score)\n",
    "    \n",
    "    # Recursively compute best scores based on transmission and emission scores at each node\n",
    "    for i in range(1, n):\n",
    "        for k, prev_y in enumerate(states):\n",
    "            for j, current_y in enumerate(states):\n",
    "                transition_key = f\"transition:{prev_y}+{current_y}\"\n",
    "                emission_key = f\"emission:{current_y}+{x[i]}\"\n",
    "                \n",
    "                # Only consider if the feature exists\n",
    "                if transition_key in feature_dict and emission_key in feature_dict:\n",
    "                    transition_score = feature_dict[transition_key]\n",
    "                    emission_score = feature_dict[emission_key]\n",
    "                    # Sum exponentials\n",
    "                    overall_score = np.exp(emission_score + transition_score) * forward_scores[i-1, k]\n",
    "\n",
    "                    # Add to forward scores array\n",
    "                    forward_scores[i, j] += overall_score\n",
    "\n",
    "    # Compute for STOP\n",
    "    forward_prob = 0\n",
    "    for j, prev_y in enumerate(states):\n",
    "        transition_key = f\"transition:{prev_y}+STOP\"\n",
    "        \n",
    "        if transition_key in feature_dict:\n",
    "            transition_score = feature_dict[transition_key]\n",
    "            # Sum exponentials\n",
    "            overall_score = np.exp(transition_score) * forward_scores[n-1, j]\n",
    "            forward_prob += overall_score\n",
    "    # End forward pass\n",
    "\n",
    "    # Start backward pass\n",
    "    # Compute STOP transition scores\n",
    "    for j, current_y in enumerate(states):\n",
    "        transition_key = f\"transition:{current_y}+STOP\"\n",
    "        if transition_key in feature_dict:\n",
    "            transition_score = feature_dict[transition_key]\n",
    "            # Sum exponentials\n",
    "            backward_scores[n-1, j] = np.exp(transition_score)\n",
    "    \n",
    "    # Recursively compute best scores based on transmission and emission scores at each node\n",
    "    for i in range(n-1, 0, -1):\n",
    "        for k, next_y in enumerate(states):\n",
    "            for j, current_y in enumerate(states):\n",
    "                transition_key = f\"transition:{current_y}+{next_y}\"\n",
    "                emission_key = f\"emission:{next_y}+{x[i]}\"\n",
    "                \n",
    "                # Only consider if the feature exists\n",
    "                if transition_key in feature_dict and emission_key in feature_dict:\n",
    "                    transition_score = feature_dict[transition_key]\n",
    "                    emission_score = feature_dict[emission_key]\n",
    "                    \n",
    "                    # Sum exponentials\n",
    "                    overall_score = np.exp(emission_score + transition_score) * backward_scores[i, k]\n",
    "                    \n",
    "                    # Add to backward scores array\n",
    "                    backward_scores[i-1, j] += overall_score\n",
    "    \n",
    "    # Compute for START\n",
    "    backward_prob = 0\n",
    "    \n",
    "    for j, next_y in enumerate(states):\n",
    "        transition_key = f\"transition:START+{next_y}\"\n",
    "        emission_key = f\"emission:{next_y}+{x[0]}\" # Emission of last word\n",
    "\n",
    "        if transition_key in feature_dict and emission_key in feature_dict:\n",
    "            transition_score = feature_dict[transition_key]\n",
    "            emission_score = feature_dict[emission_key]\n",
    "            # Sum exponentials\n",
    "            overall_score = np.exp(emission_score + transition_score) * backward_scores[0, j]\n",
    "            backward_prob += overall_score\n",
    "    # End backward pass\n",
    "    \n",
    "    # Ensure forward and backward prob are the same up to floating point errors\n",
    "    assert(abs(forward_prob - backward_prob) < 1e-8)\n",
    "        \n",
    "    # Computed expected counts for all features\n",
    "    feature_expected_counts = defaultdict(float)\n",
    "    \n",
    "    # Compute expected emission counts\n",
    "    for i in range(n):\n",
    "        for j, current_y in enumerate(states):\n",
    "            emission_key = f\"emission:{current_y}+{x[i]}\"\n",
    "            feature_expected_counts[emission_key] += forward_scores[i, j] * backward_scores[i, j] / forward_prob\n",
    "    \n",
    "    # Compute expected START / STOP transition counts\n",
    "    for j, next_y in enumerate(states):\n",
    "        start_transition_key = f\"transition:START+{next_y}\"\n",
    "        feature_expected_counts[start_transition_key] += forward_scores[0, j] * backward_scores[0, j] / forward_prob\n",
    "        \n",
    "        stop_transition_key = f\"transition:{next_y}+STOP\"\n",
    "        feature_expected_counts[stop_transition_key] += forward_scores[n-1, j] * backward_scores[n-1, j] / forward_prob\n",
    "    \n",
    "    # Compute expected transition probabilities for everything else\n",
    "    for j, current_y in enumerate(states):\n",
    "        for k, next_y in enumerate(states):\n",
    "            transition_key = f\"transition:{current_y}+{next_y}\"\n",
    "            total = 0\n",
    "            for i in range(n-1):\n",
    "                emission_key = f\"emission:{next_y}+{x[i+1]}\"\n",
    "                \n",
    "                if transition_key in feature_dict and emission_key in feature_dict:\n",
    "                    transition_score = np.exp(feature_dict[transition_key])\n",
    "                    emission_score = np.exp(feature_dict[emission_key])\n",
    "\n",
    "                    total += forward_scores[i, j] * backward_scores[i+1, k] * transition_score * emission_score / forward_prob\n",
    "            feature_expected_counts[transition_key] = total\n",
    "    \n",
    "    return feature_expected_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_count(x, y, feature_dict):\n",
    "    ''' \n",
    "    Inputs:\n",
    "        x (list[str]): Complete input word sentence (without START or STOP tags)\n",
    "        y (list[str]): Complete output label sequence\n",
    "        feature_dict (dict[str] -> float): Dictionary that maps a given feature to its score.\n",
    "    Outputs:\n",
    "        p (float): Score given by p(y | x)\n",
    "    '''\n",
    "    \n",
    "    # Input and output sequences must be of the same length\n",
    "    assert(len(x) == len(y))\n",
    "    n = len(x) # Sequence length\n",
    "    \n",
    "    # Stores the number of times each feature appears in (x, y)\n",
    "    feature_count = defaultdict(int)\n",
    "    \n",
    "    # Compute emission features\n",
    "    for i in range(n):\n",
    "        formatted_word = x[i].lower()\n",
    "        emission_key = f\"emission:{y[i]}+{formatted_word}\"\n",
    "        feature_count[emission_key] += 1\n",
    "    \n",
    "    # Compute transition features\n",
    "    # Add START and STOP tags to y\n",
    "    updated_y = [\"START\"] + y + [\"STOP\"]\n",
    "    for i in range(1, n+2):\n",
    "        prev_y = updated_y[i-1]\n",
    "        y_i = updated_y[i]\n",
    "        transition_key = f\"transition:{prev_y}+{y_i}\"\n",
    "        feature_count[transition_key] += 1\n",
    "    \n",
    "    return feature_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wish to compute the gradient:\n",
    "$\\frac{\\delta L(w)}{\\delta \\lambda_k} = \\sum_i E_{p(y|x_i)} [ f_k (x_i, y) ] - \\sum_i f_k (x_i, y_i) $\n",
    "\n",
    "which can be computed by subtracting the actual counts of the features from the expected counts of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_gradients(train_inputs, train_labels, f, states):\n",
    "    '''\n",
    "    Uses the forward-backward algorithm to compute gradients analytically.\n",
    "    \n",
    "    Inputs:\n",
    "        train_inputs (list[str]): Input sequence.\n",
    "        train_labels (list[str]): Input labels.\n",
    "        f (dict[str] -> float): Dictionary that maps a given feature to its score.\n",
    "        states (list[str]): List of unique states.\n",
    "    Outputs:\n",
    "        backward_score (float): Backward score for this sequence.\n",
    "        feature_gradients (dict[str] -> float): Dictionary that maps a given feature to its analytical gradient.\n",
    "    '''\n",
    "    \n",
    "    feature_gradients = defaultdict(float)\n",
    "\n",
    "    for i in range(len(train_labels)):\n",
    "        x = train_inputs[i]\n",
    "        y = train_labels[i]\n",
    "\n",
    "        feature_expected_counts = forward_backward(x, f, states)\n",
    "        actual_counts = get_feature_count(x, y, f)\n",
    "\n",
    "        for k, v in feature_expected_counts.items():\n",
    "            feature_gradients[k] += v\n",
    "\n",
    "        for k, v in actual_counts.items():\n",
    "            feature_gradients[k] -= v\n",
    "    \n",
    "    return feature_gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the results numerically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking emission:O+the\n",
      "Checking transition:START+O\n",
      "Checking transition:O+O\n",
      "Checking transition:I-negative+I-positive\n"
     ]
    }
   ],
   "source": [
    "train_inputs, train_labels = get_dataset(data_dir / dataset / \"train\")\n",
    "\n",
    "# Check against numerical gradient for several values is equal\n",
    "feature_key_checks = ['emission:O+the', 'transition:START+O', 'transition:O+O', 'transition:I-negative+I-positive']\n",
    "feature_gradients = compute_gradients(train_inputs, train_labels, f, states)\n",
    "loss1 = compute_crf_loss(train_inputs, train_labels, f, states)\n",
    "delta = 1e-6\n",
    "\n",
    "for feature_key in feature_key_checks:\n",
    "    print(\"Checking\", feature_key)\n",
    "    new_f = f.copy()\n",
    "    new_f[feature_key] += delta\n",
    "\n",
    "    loss2 = compute_crf_loss(train_inputs, train_labels, new_f, states)\n",
    "\n",
    "    numerical_gradient = (loss2 - loss1) / delta\n",
    "    analytical_gradient = feature_gradients[feature_key]\n",
    "    \n",
    "    # Ensure numerical and analytical gradient are close\n",
    "    assert(abs(numerical_gradient - analytical_gradient) <= 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vincent pls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
